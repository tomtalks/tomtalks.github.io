{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf73 Un cluster k8s aux petits oignons \ud83e\uddc5","text":"<p>Bienvenue dans ce merveilleux cours de cuisine de Kubernetes.</p> <p>L'objectif est de vous faire cr\u00e9er et param\u00e9trer un cluster Kubernetes from scratch pour avoir des environnements de d\u00e9veloppements, de tests aux petits oignons pour vous et vos \u00e9quipes/coll\u00e8gues.</p>"},{"location":"#cest-parti","title":"C'est parti \ud83c\udf5d","text":"<p>Pour faire simple et sans polluer votre PC, ouvrir le workspace Gitpod</p> <p>ou si vous pr\u00e9f\u00e9rez, vous pouvez cloner le repo en local dans votre r\u00e9pertoire pr\u00e9f\u00e9r\u00e9:</p> <pre><code>git clone https://gitlab.com/yodamad-workshops/2024/devoxx/kub-workshop.git\n</code></pre>"},{"location":"#prerequis-si-vous-faites-en-local","title":"Pr\u00e9requis \ud83d\udee0\ufe0f (si vous faites en local)","text":"<p>Si vous avez choisi l'option <code>Gitpod</code>, ils sont d\u00e9j\u00e0 install\u00e9s \ud83d\ude09.</p> <p>Pour ce workshop, vous aurez besoin des outils.</p> <ul> <li>git : Installation</li> <li>kubectl : Installation</li> <li>helm : Installation</li> <li>curl : Installation</li> <li>terraform : Installation</li> </ul>"},{"location":"#dernier-check-avant-de-demarrer","title":"Dernier check avant de d\u00e9marrer","text":"<p>Pour v\u00e9rifier que tout est ok et initialiser les variables d'environnement qui vont bien, nous avons pr\u00e9vu un petit script (\u00e0 faire aussi sur Gitpod)</p> <p>Si tout se d\u00e9roule comme pr\u00e9vu, vous devez avoir un r\u00e9sultat comme suit (au delta de la mise en forme suivant votre shell)</p> <pre><code>************************************************\n*    \ud83d\udc4b Bienvenue \u00e0 notre super workshop \ud83d\udc4b    *\n*  Quelques v\u00e9rifications avant de commencer   *\n************************************************\n\n\ud83d\udec2 Check local env\n    \ud83e\udd4c curl                          ... \u2705\n    \u2638\ufe0f  kubectl                       ... \u2705\n    \ud83d\ude9a helm                          ... \u2705\n    \ud83d\udcbb git                           ... \u2705\n    \ud83e\udeb4 terraform                     ... \u2705   \n\n\ud83d\udee0\ufe0f  Setup local env...\n    \ud83c\udf24\ufe0f  OVH connection setup          ... \u2705\n    \ud83c\udf0d Cloudfare setup                ... \u2705\n    \ud83e\udd8a GitLab setup                   ... \u2705\n\n************************************************\n*              \ud83e\udee1  All good !!                 *\n*       C'est parti, amusez vous bien \ud83e\udd73       *\n************************************************\n</code></pre> <p>\ud83d\udeeb Let's go ! Premi\u00e8re \u00e9tape : cr\u00e9er notre cluster \u27a1\ufe0f</p>"},{"location":"mkdocs-cheatsheet/","title":"Cheatsheet for mkdocs &amp; mkdocs-material","text":""},{"location":"mkdocs-cheatsheet/#material-theme","title":"Material theme","text":"<p>In <code>mkdocs.yml</code></p> <pre><code>theme:\n  name: material\n</code></pre>"},{"location":"mkdocs-cheatsheet/#features","title":"Features","text":"<p>In <code>mkdocs.yml</code>, enable admonitions, annotations, code highlightnings</p> <pre><code>markdown_extensions:\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - attr_list\n  - md_in_html      \n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.superfences\n  - admonition\n  - pymdownx.details\n</code></pre> <p>also for ease code snippet usage, add features in <code>theme</code> part</p> <pre><code>  features:\n    - content.code.copy\n    - content.code.annotate\n</code></pre>"},{"location":"mkdocs-cheatsheet/#usage","title":"Usage","text":""},{"location":"mkdocs-cheatsheet/#annotations","title":"Annotations","text":"<pre><code>Lorem ipsum dolor sit amet, (1) consectetur adipiscing elit.\n{ .annotate }\n\n1.  \ud83d\ude1d I'm an annotation! (1)\n    { .annotate }\n\n    1.  \ud83e\udd2f I'm an annotation in an annotation\n</code></pre>"},{"location":"mkdocs-cheatsheet/#admonitions","title":"Admonitions","text":"<p>Available keywords are : <code>note</code>, <code>abstract</code>, <code>info</code>, <code>tip</code>, <code>success</code>, <code>question</code>, <code>warning</code>, <code>failure</code>, <code>danger</code>, <code>bug</code>, <code>example</code>, <code>quote</code>. Admonition are displayed by default, but can be replace by collapsible block switching from <code>!!!</code> to <code>???</code></p> <pre><code>!!! warning\n    Beware of the wolf \ud83d\udc3a\n</code></pre> <p>It supports <code>annotation</code>:</p> <pre><code>??? bug annotate\n    this a \ud83d\udc1e (1)\n\n1.  only in a specific version\n</code></pre>"},{"location":"mkdocs-cheatsheet/#code-blocks","title":"Code blocks","text":"<p>Code blocks are of course supported, but you can also add either a title or annotation.</p> <p>\u2139\ufe0f The comment is mandatory for the annotation to be processed. Here <code>#</code> in a yaml file.</p> <pre><code>\\`\\`\\`yaml\n  annotations: # (1)\n    downscaler/uptime: Mon 07:30-18:00 CET\n    downscaler/force-downtime: \"true\"\n\\`\\`\\`\n\n1. \ud83e\udd76 Incredible annotation   \n</code></pre>"},{"location":"mkdocs-cheatsheet/#snippets","title":"Snippets","text":"<p>We can include files as snippets, with an optional title:</p> <pre><code>\\```yaml title=\"a simple deployment sample\"\n\\```\n</code></pre>"},{"location":"mkdocs-cheatsheet/#build","title":"Build","text":"<p>Pour installer mkdocs et les extensions:</p>"},{"location":"mkdocs-cheatsheet/#avec-un-virtualenv","title":"Avec un virtualenv","text":"<pre><code>python3 -m venv ./.venv\nsource .venv/bin/activate\n</code></pre> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Pour lancer le serveur de dev:</p> <pre><code>mkdocs serve --clean\n</code></pre> <p>Pour lancer un build et g\u00e9n\u00e9rer le pdf <pre><code>mkdocs build\n</code></pre></p>"},{"location":"recipe/","title":"Notre recette d'un cluster kub aux petits oignons","text":""},{"location":"recipe/#la-recette","title":"La recette","text":"<p>Pour avoir un bon cluster:</p> <ul> <li>on l'instancie avec terraform</li> <li>on g\u00e8re l'acc\u00e8s \u00e0 nos endpoints depuis l'ext\u00e9rieur avec un ingress-controller tel que nginx</li> <li>pour que ca soit lisible, on cr\u00e9e automatique des entr\u00e9es DNS pour chaque nouveau endpoint avec external-dns</li> <li>on s\u00e9curise tout \u00e7a avec des jolis certificats Let's Encrypt et \u00e0 l'aide de cert-manager</li> <li>on externalise nos secrets dans GitLab avec external-secrets</li> <li>on contr\u00f4le un peu tout ce qu'il se passe dans la cocotte avec kyverno</li> <li>une fois qu'on a bien travaill\u00e9, on autorise tout le monde \u00e0 se reposer avec kube-downscaler</li> </ul> <p>Miam \ud83e\udd24</p>"},{"location":"recipe/#et-ca-sert-a-quoi","title":"Et ca sert \u00e0 quoi \u2049\ufe0f","text":"<p>Dans la vraie vie, ca nous sert \u00e0 quoi.</p> <p>Un premier exemple via l'utilisation des environnements GitLab pour simplement visualiser les diff\u00e9rentes environnements d'un projet, quelle version/commit est d\u00e9ploy\u00e9 sur chacun, et faciliter leur listing et leur acc\u00e8s.</p> <p>Aussi, on peut voir qu'avec une approche GitOps et Flux par exemple, on peut facilement automatiser l'installation et la configuration de tout cela et de simplement mettre \u00e0 jour tous nos clusters.</p> <p>Essayons tout cela...</p>"},{"location":"recipe/#cest-parti","title":"C'est parti \ud83c\udf5d","text":"<p>Pour ces 2 exemples, il est n\u00e9cessaire de forker le projet car vous aurez besoin d'\u00eatre Owner du projet pour faire les manipulations.</p> <p>Les 2 exercices sont ind\u00e9pendants, vous pouvez les faire dans l'ordre que vous souhaitez.</p> <p>Pour cel\u00e0, il suffit de cliquer sur <code>Fork</code> sur la page du projet et r\u00e9aliser le fork de ce projet dans votre espace personnel.</p> <p></p> <p>Il en reste plus qu'a cloner votre fork en local !</p> <ul> <li>Pour la recette des environnements Gitlab \u27a1\ufe0f.</li> <li>Pour la recette avec FluxCD \u27a1\ufe0f.</li> </ul>"},{"location":"recipe/#avant-de-partir-apres-avoir-fait-les-parties-gitlab-env-etou-flux","title":"Avant de partir \ud83e\uddfd (APRES avoir fait les parties GitLab env. et/ou Flux)","text":"<p>Pensez \u00e0 faire du m\u00e9nage \ud83e\uddf9</p> <p>1\ufe0f\u20e3 Supprimer les ingress avant de supprimer le cluster (pour purger les records DNS):</p> <pre><code>for i in $(kubectl get ingress -A --no-headers -o=name); do\n  kubectl delete $i -n demos\ndone\n</code></pre> <p>2\ufe0f\u20e3 Supprimer le cluster:</p> <pre><code>cd terraform\nterraform destroy \n</code></pre>"},{"location":"cert-manager/","title":"S\u00e9curit\u00e9 avant tout","text":"<p>Nous allons g\u00e9rer les certificats avec cert-manager</p>"},{"location":"cert-manager/#installation","title":"Installation","text":"<p><code>cert-manager</code> fournit un Helm chart, configurons le repo:</p> <pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\n</code></pre> <p>Et maintenant nous pouvons installer <code>cert-manager</code> dans notre cluster dans un namespace d\u00e9di\u00e9</p> <pre><code>helm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.13.3 \\\n  --set installCRDs=true\n</code></pre> <p>cert-manager est install\u00e9</p> <pre><code>cert-manager v1.13.3 has been deployed successfully! # (1)\n</code></pre> <ol> <li>\u2139\ufe0f La version peut \u00eatre diff\u00e9rente</li> </ol> <p>Maintenant, v\u00e9rifions que tout est ok:</p> <pre><code>kubectl get all -n cert-manager\n</code></pre> <pre><code>NAME                                         READY   STATUS    RESTARTS   AGE\npod/cert-manager-6856dc897b-k6dks            1/1     Running   0          32s\npod/cert-manager-cainjector-c86f8699-cmc7t   1/1     Running   0          32s\npod/cert-manager-webhook-f8f64cb85-7rjjz     1/1     Running   0          32s\n\nNAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nservice/cert-manager           ClusterIP   10.3.242.74    &lt;none&gt;        9402/TCP   33s\nservice/cert-manager-webhook   ClusterIP   10.3.182.243   &lt;none&gt;        443/TCP    33s\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/cert-manager              1/1     1            1           33s\ndeployment.apps/cert-manager-cainjector   1/1     1            1           33s\ndeployment.apps/cert-manager-webhook      1/1     1            1           33s\n\nNAME                                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/cert-manager-6856dc897b            1         1         1       33s\nreplicaset.apps/cert-manager-cainjector-c86f8699   1         1         1       33s\nreplicaset.apps/cert-manager-webhook-f8f64cb85     1         1         1       33s\n</code></pre>"},{"location":"cert-manager/#configuration","title":"Configuration","text":"<p><code>cert-manager</code> permet de g\u00e9n\u00e9rer des certificats pour s\u00e9curiser nos endpoints. Il supporte plusieurs providers, ici nous allons utiliser le classique mais pratique Let's Encrypt.</p> <p>Pour cela, nous devons cr\u00e9er un <code>Issuer</code>, ici un <code>ClusterIssuer</code> pour acc\u00e9der \u00e0 l'ensemble du cluster</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-production\n  namespace: cert-manager\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: &lt;your-email&gt; # (1)\n    privateKeySecretRef:\n      name: letsencrypt-production # (2)\n    solvers:\n      - http01:\n          ingress:\n            class: nginx # (3)\n</code></pre> <ol> <li>\ud83d\udce7 Un email est n\u00e9cessaire pour l'appartenance du domaine</li> <li>\ud83d\udcdc On utilise le serveur de production</li> <li>\ud83d\udedc On pr\u00e9cise que l'on r\u00e9pondra au challenge HTTP en utilisant un  <code>Ingress</code> de type <code>nginx</code></li> </ol> <p>Editer le fichier <code>cert-manager/letsencrypt-cluster-issuer.yml</code> pour positionner un email valide.</p> <p>Cr\u00e9ons notre issuer:</p> <pre><code>kubectl apply -n cert-manager -f cert-manager/letsencrypt-cluster-issuer.yml\n</code></pre> <p>Issuer cr\u00e9\u00e9</p> <pre><code>clusterissuer.cert-manager.io/letsencrypt-production created\n</code></pre> Utilisation du serveur de staging <p>Par d\u00e9faut, on utilise le serveur de production de Let's Encrypt, mais celui \u00e0 la limitation que si vous avez un nombre de requ\u00eates en \u00e9chec trop importants, vous pouvez \u00eatre banni temporairement.</p> <p>Pour une phase de test, il est possible d'utiliser le serveur de staging de Let's Encrypt qui n'a pas cette limitation.</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\n  namespace: cert-manager\nspec:\n  acme:\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    email: &lt;your-email&gt;\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre>"},{"location":"cert-manager/#utilisation","title":"Utilisation","text":"<p>Maintenant, on peut s\u00e9curiser nos URLs. Il suffit d'updater notre <code>Ingress</code> pour lui configurer le TLS.</p> <p>Pour cela, il faut mettre \u00e0 jour le fichier <code>demos/deployment-with-ingress-https-grunty.yml</code> avec votre trigramme.</p> <p>D\u00e9ployons ensuite notre nouveau composant:</p> <pre><code>kubectl apply -f demos/deployment-with-ingress-https-grunty.yml\n</code></pre> Pour les curieux, le contenu du manifest <code>deployment-with-ingress-https-grunty.yml</code> <p>Les lignes surlign\u00e9es configurent le TLS <pre><code>\n</code></pre></p> <p>Deployment install\u00e9</p> <pre><code>ingress.networking.k8s.io/secured-deployment-ingress created\n</code></pre> <p>On peut voir que <code>cert-manager</code> instancie un <code>Ingress</code> pour g\u00e9rer les \u00e9changes avec Let's Encrypt pour la g\u00e9n\u00e9ration du certificat:</p> <pre><code>kubectl get ingress -n demos\n</code></pre> <pre><code>NAME                         CLASS    HOSTS                                       ADDRESS         PORTS     AGE\ncm-acme-http-solver-hmh8t    &lt;none&gt;   secured.&lt;votre_trigramme&gt;.grunty.uk                         80        47s\nnew-deployment-ingress       nginx    new-deployment.&lt;votre_trigramme&gt;.grunty.uk  57.128.120.31   80        19h\nsecured-deployment-ingress   nginx    secured.&lt;votre_trigramme&gt;.grunty.uk                         80, 443   50s\n</code></pre> <p>Une fois les challanges termin\u00e9s avec Let's Encrypt, on voit aussi qu'un <code>Certificate</code> a \u00e9t\u00e9 g\u00e9n\u00e9r\u00e9. Il est au statut <code>Ready</code> \u00e0 <code>False</code>, au bout de quelques secondes, il devrait passer \u00e0 <code>True</code></p> <pre><code>kubectl get certificates -n demos\n</code></pre> <p>Certificat g\u00e9n\u00e9r\u00e9</p> <pre><code>NAME                                      READY   SECRET                                    AGE\nsecured.&lt;votre_trigramme&gt;.grunty.uk-tls   True    secured.&lt;votre_trigramme&gt;.grunty.uk-tls   3m44s\n</code></pre> Behind the scene <p>On peut facilement voir les \u00e9tapes en observant les events</p> <pre><code>kubectl get events --sort-by='.lastTimestamp' -n demos\n</code></pre> <p>Tout semble ok, on peut v\u00e9rifier que notre URL est d\u00e9sormais s\u00e9curis\u00e9e :  https://secured.{votre_trigramme}.grunty.uk</p> <p>Site accessible en HTTPs</p> <p>Mais notre chef de brigade est vraiment pointilleux \ud83d\ude05, notre gestion des donn\u00e9es sensibles tels que nos API keys ne lui convient pas.</p> <p>Alors pimentons un peu tout cela avec une gestion de secrets plus propre \u27a1\ufe0f</p>"},{"location":"external-dns/","title":"Gestion des URLs","text":"<p>Avant d'installer <code>external-dns</code>, il est n\u00e9cessaire de cr\u00e9er un <code>ServiceAccount</code> afin de lui donner acc\u00e8s aux diff\u00e9rents namespaces pour d\u00e9tecter les nouveaux endpoints \u00e0 g\u00e9rer:</p> <pre><code>kubectl apply -f external-dns/external-dns-rbac.yml\n</code></pre> <p>ServiceAccount cr\u00e9\u00e9</p> <pre><code>namespace/external-dns created\nserviceaccount/external-dns created\nclusterrole.rbac.authorization.k8s.io/external-dns created\nclusterrolebinding.rbac.authorization.k8s.io/external-dns-viewer created\n</code></pre>"},{"location":"external-dns/#configuration-de-external-dns-pour-cloudflare","title":"Configuration de external-dns pour CloudFlare","text":"<p>Vous aurez besoin de 2 infos : la cl\u00e9 d'API et le user email r\u00e9f\u00e9renc\u00e9 par Cloudfare. Ces 2 infos sont stock\u00e9s dans des variables d'environnement <code>API_KEY</code> &amp; <code>API_MAIL</code></p> <p>Elles sont d\u00e9j\u00e0 configur\u00e9es</p> <p>On est sympa, c'est d\u00e9j\u00e0 fait gr\u00e2ce au script ex\u00e9cut\u00e9 au d\u00e9but du workshop. Si vous avez changer de terminal, il faut refaire la commande suivante: <pre><code>\n</code></pre></p>"},{"location":"external-dns/#gestion-du-secret-pour-acceder-a-lapi-cloudflare","title":"Gestion du secret pour acc\u00e9der \u00e0 l'API Cloudflare","text":"<p>Il est n\u00e9cessaire de cr\u00e9er un secret pour stocker l'API key d'acc\u00e8s \u00e0 Cloudflare et un autre pour le compte de connexion</p> <pre><code>kubectl -n external-dns create secret generic cloudflare-api-token --from-literal=api-key=$API_KEY\nkubectl -n external-dns create secret generic cloudflare-user-mail --from-literal=user-mail=$API_MAIL\n</code></pre> <p>On peut v\u00e9rifier que les secrets sont bien cr\u00e9\u00e9s et disponibles</p> <pre><code>kubectl get secrets -n external-dns\n</code></pre> <p>Secrets cr\u00e9\u00e9s</p> <pre><code>NAME                   TYPE     DATA   AGE\ncloudflare-api-token   Opaque   1      15s\ncloudflare-user-mail   Opaque   1      11s\n</code></pre>"},{"location":"external-dns/#installation-dexternal-dns","title":"Installation d'external-dns","text":"<p>Pour d\u00e9ployer <code>external-dns</code>, il suffit de cr\u00e9er un <code>Deployment</code> installant l'image officielle d'<code>external-dns</code></p> <pre><code>kubectl apply -f external-dns/external-dns-cloudflare.yml\n</code></pre> <p>Pour comprendre le manifest:</p> <pre><code>\n</code></pre> Documentation officielle Cloudflare <p>La documentation officielle pour Cloudflare est dispo</p> <p><code>external-dns</code> is Running</p> <p><pre><code>kubectl get po -n external-dns\n</code></pre> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\nexternal-dns-cloudflare-694f6f75-nf8n8   1/1     Running   0          2s\n</code></pre></p> V\u00e9rifier les logs <p>On peut aussi v\u00e9rifier que la connexion \u00e0 Cloudflare est bien ok</p> <pre><code>kubectl logs $(kubectl get po -n external-dns | grep external-dns-cloudflare | cut -d' ' -f1) -n external-dns\n</code></pre> <pre><code>...\ntime=\"2023-12-20T08:52:41Z\" level=info msg=\"Instantiating new Kubernetes client\"\ntime=\"2023-12-20T08:52:41Z\" level=info msg=\"Using inCluster-config based on serviceaccount-token\"\ntime=\"2023-12-20T08:52:41Z\" level=info msg=\"Created Kubernetes client https://10.3.0.1:443\"\n...\n</code></pre>"},{"location":"external-dns/#external-dns-en-action","title":"External DNS en action","text":"<p>On peut voir dans les logs qu'<code>external-dns</code> a d\u00e9j\u00e0 automatiquement d\u00e9tect\u00e9 notre <code>Ingress</code> pr\u00e9c\u00e9dent:</p> <pre><code>kubectl logs -f $(kubectl get po -n external-dns | grep external-dns-cloudflare | cut -d' ' -f1) -n external-dns\n</code></pre> <pre><code>...\ntime=\"2023-12-20T08:52:44Z\" level=info msg=\"Changing record.\" action=CREATE record=new-deployment.mvt.grunty.uk ttl=1 type=A zone=be73d3e4c087b970da9bb670130a11fc\ntime=\"2023-12-20T08:52:45Z\" level=info msg=\"Changing record.\" action=CREATE record=new-deployment.mvt.grunty.uk ttl=1 type=TXT zone=be73d3e4c087b970da9bb670130a11fc\ntime=\"2023-12-20T08:52:45Z\" level=info msg=\"Changing record.\" action=CREATE record=a-new-deployment.mvt.grunty.uk ttl=1 type=TXT zone=be73d3e4c087b970da9bb670130a11fc\n...\n</code></pre> <p>On peut v\u00e9rifier que notre nom de domaine est bien reconnu</p> <pre><code>dig new-deployment.$TF_VAR_OVH_CLOUD_PROJECT_KUBE_NAME.grunty.uk @ara.ns.cloudflare.com\n</code></pre> <p>DNS est configur\u00e9 avec notre IP publique</p> <pre><code>;; ANSWER SECTION:\nnew-deployment.&lt;votre_trigramme&gt;.grunty.uk. 300 IN  A   57.128.120.31 (1)\n</code></pre> <ol> <li>\ud83d\udedc l'IP sera diff\u00e9rente</li> </ol> <p>On a chang\u00e9 l'image Docker entre temps</p> <p>Ne vous \u00e9tonnez pas si vous ne voyez plus le magnifique cin\u00e9ma ASCII du d\u00e9part.</p> <p>On a chang\u00e9 l'image Docker pour que ce que l'on affiche soit lisible lorsque l'on fait un cUrl</p> <p>On peut aussi valider que le navigateur reconnait notre URL en visitant http://new-deployment.{votre_trigramme}.grunty.uk/</p> <p>ou via un cURL comme \u00e0 l'\u00e9tape pr\u00e9c\u00e9dente</p> <pre><code>curl new-deployment.&lt;votre_trigramme&gt;.grunty.uk\n</code></pre> <pre><code>Server address: 10.2.1.6:80\nServer name: new-deployment-5998d8dbcc-kdbrk\nDate: 20/Dec/2023:09:45:13 +0000\nURI: /\nRequest ID: 82d85003846eed6c62744103d7ac2bda\n</code></pre> <p>C'est beaucoup plus pratique !! \ud83e\udd73</p> <p>Mais ... ce n'est pas tr\u00e8s s\u00e9curis\u00e9 le HTTP, le chef de brigade de la s\u00e9curit\u00e9 nous rappelle \u00e0 l'ordre \ud83e\udee3</p> <p>Si on ajoute quelques mL de s\u00e9curit\u00e9 avec des certificats pour notre HTTPs \u27a1\ufe0f</p>"},{"location":"external-secrets/","title":"S\u00e9curisation des secrets","text":"<p>Pour s\u00e9curiser nos secrets, nous allons utiliser <code>external-secrets</code> pour simplifier la d\u00e9l\u00e9gation de la gestion des secrets \u00e0 un outil tiers tel que GitLab, HashiCorp Vault...</p>"},{"location":"external-secrets/#installation","title":"Installation","text":"<p><code>external-secrets</code> fournit un Helm chart pour une installation simple</p> <p>On ajoute donc le repo pour <code>external-secrets</code> :</p> <pre><code>helm repo add external-secrets https://charts.external-secrets.io\nhelm repo update\n</code></pre> <p>et on installe dans un namespace d\u00e9di\u00e9 \ud83d\ude80</p> <pre><code>helm install external-secrets external-secrets/external-secrets -n external-secrets --create-namespace --set installCRDs=true\n</code></pre> <p>external-secrets est install\u00e9</p> <pre><code>external-secrets has been deployed successfully!\n</code></pre> <p><code>external-secrets</code> instancie plusieurs composants, on peut les lister:</p> <pre><code>kubectl get all -n external-secrets\n</code></pre> <pre><code>NAME                                                   READY   STATUS    RESTARTS   AGE\npod/external-secrets-5f45b6f844-27fmq                  1/1     Running   0          2m9s\npod/external-secrets-cert-controller-9795887f6-8mssr   1/1     Running   0          2m9s\npod/external-secrets-webhook-6f4789ccf-qmpkn           1/1     Running   0          2m9s\n\nNAME                               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\nservice/external-secrets-webhook   ClusterIP   10.3.175.44   &lt;none&gt;        443/TCP   2m11s\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/external-secrets                   1/1     1            1           2m10s\ndeployment.apps/external-secrets-cert-controller   1/1     1            1           2m10s\ndeployment.apps/external-secrets-webhook           1/1     1            1           2m10s\n\nNAME                                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/external-secrets-5f45b6f844                  1         1         1       2m9s\nreplicaset.apps/external-secrets-cert-controller-9795887f6   1         1         1       2m9s\nreplicaset.apps/external-secrets-webhook-6f4789ccf           1         1         1       2m9s\n</code></pre> <p>Tout est pr\u00eat !</p>"},{"location":"external-secrets/#configuration","title":"Configuration","text":"<p>Pour permettre \u00e0 <code>external-secrets</code> de g\u00e9rer les secrets, il est n\u00e9cessaire de cr\u00e9er un <code>SecretStore</code>. Celui-ci fera le lien entre le cluster et l'outil h\u00e9bergeant les secrets.</p> <p>Pour faire simple dans ce workshop, on va faire avec GitLab pour stocker nos donn\u00e9es sensibles via l'utilisation des variables de CICD. C'est simple \u00e0 mettre en oeuvre mais introduit une faiblesse cot\u00e9 s\u00e9curit\u00e9 car n\u00e9cessite quand m\u00eame d'avoir un secret cot\u00e9 Kubernetes, pour stocker la cl\u00e9 d'API pour se connecter \u00e0 GitLab.</p> D'autres solutions sont possibles... <p><code>external-secrets</code> fournit plusieurs connecteurs possibles. Le plus complet semble \u00eatre celui d'HashiCorp Vault car il n'introduit pas de besoin de cl\u00e9 ou de credentials pour s'int\u00e9grer dans le cluster.</p> <p>On a d\u00e9j\u00e0 pr\u00e9par\u00e9 les variables dans le projet GitLab (via le menu <code>Settings -&gt; CICD -&gt; Variables</code>):</p> <p></p> <p>Cr\u00e9ons le secret de connexion:</p> <pre><code>kubectl -n external-secrets create secret generic gitlab-token --from-literal=token=$GITLAB_TOKEN\n</code></pre> <p>GITLAB_TOKEN est d\u00e9j\u00e0 configur\u00e9 pour vous</p> <p>On est sympa, c'est d\u00e9j\u00e0 fait gr\u00e2ce au script ex\u00e9cut\u00e9 au d\u00e9but du workshop. Si vous avez changer de terminal, il faut refaire la commande suivante: <pre><code>\n</code></pre></p> <p>Maintenant on peut cr\u00e9er un <code>ClusterSecretStore</code> pour faire le lien entre GitLab et le cluster.</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: gitlab-cluster-secret-store\n  namespace: external-secrets\nspec:\n  provider:\n    # provider type: gitlab\n    gitlab: # (1)\n      url: https://gitlab.com/ # (2)\n      auth:\n        SecretRef:\n          accessToken:\n            name: gitlab-token\n            key: token\n            namespace: external-secrets\n      projectID: \"53147568\" # (3)\n</code></pre> <ol> <li>\ud83d\udc7d Le type du provider</li> <li>\ud83d\udd17 L'URL de votre GitLab, pas forc\u00e9ment gitlab.com</li> <li>\ud83d\udce6 L'ID du projet qui h\u00e9berge les donn\u00e9es sensibles</li> </ol> Alternative <code>SecretStore</code> <p>Il est possible d'utiliser un <code>SecretStore</code> pour restreindre \u00e0 un namespace, si vous voulez limiter l'accessibilit\u00e9/utilisation de vos secrets.</p> <p>On peut d\u00e9ployer notre <code>ClusterSecretStore</code>:</p> <pre><code>kubectl apply -f external-secrets/external-secrets-secret-store.yml -n external-secrets\n</code></pre> Pour les curieux, le fichier <code>external-secrets-secret-store.yml</code> <pre><code>\n</code></pre> <p>et v\u00e9rifier que le connexion \u00e0 GitLab est op\u00e9rationnelle:</p> <pre><code>kubectl get clustersecretstores.external-secrets.io -n external-secrets\n</code></pre> <p>ClusterSecretStore est valide</p> <pre><code>NAME                          AGE     STATUS   CAPABILITIES   READY\ngitlab-cluster-secret-store   2m50s   Valid    ReadOnly       True\n</code></pre>"},{"location":"external-secrets/#configuration-dun-externalsecret","title":"Configuration d'un <code>ExternalSecret</code>","text":"<p>Nos secrets stock\u00e9s dans GitLab, il faut cr\u00e9er d\u00e9sormais cr\u00e9er un <code>ExternalSecret</code> qui va rappatrier dans notre cluster le secret stock\u00e9 dans GitLab.</p> <p>On va faire le test sur <code>external-dns</code> pour externaliser notre cl\u00e9 d'API et username Cloudflare dans GitLab:</p> <pre><code>kubectl apply -n external-dns -f external-secrets/external-secrets-external-secret.yml\n</code></pre> Pour les curieux, le fichier <code>external-secrets-external-secret.yml</code> <pre><code>\n</code></pre> <p>Les secrets sont cr\u00e9\u00e9s</p> <pre><code>externalsecret.external-secrets.io/external-secret-cloudflare-key-credentials created\nexternalsecret.external-secrets.io/external-secret-cloudflare-mail-credentials created\n</code></pre> <p>On peut v\u00e9rifier que nos secrets sont valides (ie synchroniz\u00e9s \u00e0 l'\u00e9tat <code>SecretSynced</code>):</p> <pre><code>kubectl get externalsecrets.external-secrets.io -n external-dns\n</code></pre> <p>Success</p> <pre><code>NAME                                          STORE                         REFRESH INTERVAL   STATUS         READY\nexternal-secret-cloudflare-key-credentials    gitlab-cluster-secret-store   1h                 SecretSynced   True\nexternal-secret-cloudflare-mail-credentials   gitlab-cluster-secret-store   1h                 SecretSynced   True\n</code></pre> <p>Automatiquement <code>external-secrets</code> a g\u00e9n\u00e9r\u00e9 des secrets pour nous:</p> <pre><code>kubectl get secrets -n external-dns\n</code></pre> <p>Les nouveaux secrets sont g\u00e9n\u00e9r\u00e9s</p> <pre><code>NAME                            TYPE     DATA   AGE\ncloudflare-api-token            Opaque   1      4h53m\ncloudflare-user-mail            Opaque   1      15h\nexternal-cloudflare-api-token   Opaque   1      99s\nexternal-cloudflare-user-mail   Opaque   1      98s\n</code></pre>"},{"location":"external-secrets/#configuration-de-external-dns-pour-utiliser-les-nouveaux-secrets","title":"Configuration de <code>external-dns</code> pour utiliser les nouveaux secrets","text":"<p>Maintenant que l'on a nos nouveaux secrets, on peut mettre \u00e0 jour notre <code>Deployment</code> d'<code>external-dns</code> pour qu'il les utilise plut\u00f4t que les anciens.</p> <p>On change donc la r\u00e9f\u00e9rence des secrets dans le <code>Deployment</code>:</p> <pre><code>- name: CF_API_TOKEN\n  valueFrom:\n    secretKeyRef:\n      name: external-cloudflare-api-token\n      key: api-key\n- name: CF_API_EMAIL\n  valueFrom:\n    secretKeyRef:\n      name: external-cloudflare-user-mail\n      key: user-mail\n</code></pre> <p>On aura plus besoin de nos anciens secrets, autant les supprimer:</p> <pre><code>kubectl delete secret cloudflare-api-token cloudflare-user-mail -n external-dns\n</code></pre> <p>Et on met \u00e0 jour <code>external-dns</code> pour utiliser les nouveaux</p> <pre><code>kubectl apply -f external-secrets/external-dns-cloudflare.yml -n external-dns\n</code></pre> Pour les curieux, le fichier <code>external-dns-cloudflare.yml</code> <pre><code>\n</code></pre> <p>Apr\u00e8s quelques secondes, on voit qu'un nouveau pod a \u00e9t\u00e9 cr\u00e9\u00e9 et fonctionne:</p> <pre><code>kubectl get po -n external-dns\n</code></pre> <p>external-dns est recr\u00e9\u00e9 et op\u00e9rationnel</p> <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE\nexternal-dns-XXX-7d4cf677f6-nmwn5     1/1     Running   0          25s\n</code></pre> <p>et on v\u00e9rifie que la connexion avec Cloudflare est toujours ok:</p> <pre><code>kubectl logs -f $(kubectl get pods -l \"app.kubernetes.io/name=external-dns\" -n external-dns | grep external-dns-cloudflare | cut -d' ' -f1) -n external-dns\n</code></pre> <p>external-dns est bien connect\u00e9 \u00e0 Cloudflare</p> <pre><code>time=\"2023-12-20T13:48:56Z\" level=info msg=\"Instantiating new Kubernetes client\"\ntime=\"2023-12-20T13:48:56Z\" level=info msg=\"Using inCluster-config based on serviceaccount-token\"\ntime=\"2023-12-20T13:48:56Z\" level=info msg=\"Created Kubernetes client https://10.3.0.1:443\"\n</code></pre> <p>Le chef de brigade s\u00e9curit\u00e9 est content, mais il sent qu'on a du r\u00e9pondant \ud83d\ude0e, il nous impose alors encore de nouvelles contraintes : il ne vaut que des ingr\u00e9dients contr\u00f4l\u00e9s en amont et veut favoriser la fili\u00e8re locale plutot que la grande distribution... \ud83d\uded2</p> <p>Ok, challenge accepted !! \ud83d\udcaa Montrons-lui comment faire \u27a1\ufe0f</p>"},{"location":"flux/","title":"GitOps tu connais ?","text":"<p>Ok, on a r\u00e9ussi notre petite recette. Tout est \u00e9crit mais \u00e7a serait encore mieux si on pouvait automatiser tout \u00e7a en utilisant Flux par exemple.</p>"},{"location":"flux/#pre-requis","title":"Pr\u00e9-requis","text":"<p>Pour boostraper Flux (en gros l'installer dans votre cluster), il faut que la personne qui lance la commande ait les droits cluster admin sur le cluster Kubernetes cible. Il est aussi n\u00e9cessaire que la personne qui lance la commande soit le propri\u00e9taire du projet GitLab, ou ait les droits admin d'un groupe GitLab.</p> <p>N'oubliez pas de forker le projet pour pouvoir le modifier ET de basculer sur votre nouveau repo \ud83d\ude01</p> <p>Info</p> <p>Si vous avez fait la partie environnements gitlab avant celle-ci et que vous avez d\u00e9j\u00e0 fork\u00e9 le projet, pas besoin de faire un nouveau fork. Vous pouvez r\u00e9utiliser le 1er.</p> <p>Info</p> <p>Si vous utilisez Gitpod n'oubliez pas de t\u00e9l\u00e9charger votre <code>kubeconfig</code> alias <code>cluster-ovh-${TF_VAR_OVH_CLOUD_PROJECT_KUBE_NAME}.yml</code>. Avec le fork vous allez d\u00e9marrer avec un nouveau pod et par cons\u00e9quence vos fichiers locaux ne seront plus accessibles.</p> <p>Ensuite, il va nous falloir un token GitLab pour que Flux puisse se connecter \u00e0 notre repo GitLab.</p>"},{"location":"flux/#gitlab-pat-personal-access-token","title":"GitLab PAT (Personal Access Token)","text":"<p>On va r\u00e9cup\u00e9rer le token et le stocker dans une variable d'environnement.</p> Comment r\u00e9cup\u00e9rer un token GitLab <p>Pour cr\u00e9er un token GitLab, il faut aller dans votre profil GitLab, puis dans <code>Preferences &gt; Access Tokens</code> </p> <p></p> <p>Et cr\u00e9er un token avec les bons scopes: </p> <pre><code>export GITLAB_TOKEN=&lt;THE TOKEN&gt;\n</code></pre>"},{"location":"flux/#installation-de-flux","title":"Installation de Flux","text":"<p>La premi\u00e8re \u00e9tape est d'installer Flux CLI.</p> <pre><code>curl -s https://fluxcd.io/install.sh | sudo bash\n</code></pre>"},{"location":"flux/#cette-fois-cest-la-bonne-on-configure-flux","title":"Cette fois c'est la bonne : on configure Flux","text":"<p>On va demander \u00e0 la CLI d'initialiser Flux sur notre cluster et de se connecter \u00e0 notre repo GitLab.</p> <p>Tout est d\u00e9crit dans le tuto Flux.</p> <p>Info</p> <p>Si la variable d'environement GITLAB_TOKEN n'est pas renseign\u00e9e, le boostrap va demander de saisir le token.</p> <p>Il est possible de fournir le token avec une commande du type: <code>echo \"&lt;gl-token&gt;\" | flux bootstrap gitlab</code>.</p> <p>On lance le bootstrap sur le projet avec notre compte personnel:</p> <pre><code>flux bootstrap gitlab \\\n  --deploy-token-auth \\\n  --owner=&lt;NAMESPACE_NAME&gt; \\\n  --repository=&lt;PROJECT_STUG&gt; \\\n  --branch=main \\\n  --path=flux/devoxx-cluster/ \\\n  --personal\n</code></pre> <p>Il y a trois param\u00e8tres \u00e0 remplacer :</p> <ul> <li>NAMESPACE_NAME : le groupe ou sous-groupe dans lequel vous voulez initialiser Flux</li> <li>PROJECT_STUG : le project dans lequel Flux va stocker les informations dont il a besoin</li> <li>path : le chemin o\u00f9 sont stock\u00e9 les fichiers de configuration de Flux dans le repo</li> </ul> <p>NAMESPACE_NAME + PROJECT_STUG doivent correspondre au chemin du repository dans lequel vous avez fork\u00e9 ce workshop.</p> <p>Lorsque l'on utilise <code>--deploy-token-auth</code>, la CLI g\u00e9n\u00e8re un token GL et le stock dans le cluster sous la forme d'une <code>Secret</code> qui s'appelle flux-system dans le Namespace flux-system.</p> Flux bootstrap output <pre><code>\u25ba connecting to https://gitlab.com\n\u25ba cloning branch \"main\" from Git repository \"https://gitlab.com/yodamad-workshops/kub-workshop.git\"\n\u2714 cloned repository\n\u25ba generating component manifests\n\u2714 generated component manifests\n\u2714 committed sync manifests to \"main\" (\"4271d8d7adef5572f1031f0f21767d449d0ccbb4\")\n\u25ba pushing component manifests to \"https://gitlab.com/yodamad-workshops/kub-workshop.git\"\n\u25ba installing components in \"flux-system\" namespace\n\u2714 installed components\n\u2714 reconciled components\n\u25ba checking to reconcile deploy token for source secret\n\u2714 configured deploy token \"flux-system-main-flux-system-./flux/devoxx-cluster\" for \"https://gitlab.com/yodamad-workshops/kub-workshop\"\n\u25ba determining if source secret \"flux-system/flux-system\" exists\n\u25ba generating source secret\n\u25ba applying source secret \"flux-system/flux-system\"\n\u2714 reconciled source secret\n\u25ba generating sync manifests\n\u2714 generated sync manifests\n\u2714 committed sync manifests to \"main\" (\"05dfd597d5959fda3a783f2336b65d1f1d7b121d\")\n\u25ba pushing sync manifests to \"https://gitlab.com/yodamad-workshops/kub-workshop.git\"\n\u25ba applying sync manifests\n\u2714 reconciled sync configuration\n\u25ce waiting for Kustomization \"flux-system/flux-system\" to be reconciled\n\u2714 Kustomization reconciled successfully\n\u25ba confirming components are healthy\n\u2714 helm-controller: deployment ready\n\u2714 kustomize-controller: deployment ready\n\u2714 notification-controller: deployment ready\n\u2714 source-controller: deployment ready\n\u2714 all components are healthy\n</code></pre> <p>Bien jou\u00e9 !</p> <p>L'agent Flux va maintenant surveiller notre repo GitLab et appliquer les changements automatiquement.</p>"},{"location":"flux/#petite-visite-de-notre-nouvelle-cuisine","title":"Petite visite de notre nouvelle cuisine","text":""},{"location":"flux/#le-cellier","title":"Le cellier","text":"<p>Si on pull le repo GitLab, on peut voir que Flux a cr\u00e9e un nouveau repertoire <code>flux/devoxx-cluster/flux-system</code> qui contient la configuration de Flux :</p> <ul> <li><code>kustomization.yaml</code> est un index, on va lister ici les manifests qui doivent \u00eatre pris en compte dans ce r\u00e9pertoire.</li> <li><code>gotk-components.yaml</code> contient la d\u00e9finition des RBAC et des CRDs (Custom Resource Definition) utilis\u00e9es par Flux.</li> <li><code>gotk-sync.yaml</code> d\u00e9finit la mani\u00e8res dont l'op\u00e9rateur se connecte au repo au travers du Kind <code>GitRepository</code> et le type Kustomization permet de configurer quels sont les manifest / configuration \u00e0 scruter. Pour nous, tout ce qui se trouve dans <code>./flux/devoxx-cluster</code> sera utilis\u00e9 comme configuration.</li> </ul> <p>Ici le fichier <code>./flux/devoxx-cluster/flux-system/gotk-sync.yaml</code> contient la configuration de Flux pour se connecter \u00e0 notre repo GitLab.</p> ./flux/devoxx-cluster/flux-system/gotk-sync.yaml<pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n# ICI on trouve le nom de notre Objet GitRepository\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 1m0s\n  ref:\n# ICI la branche \u00e0 utiliser\n    branch: main\n  secretRef:\n    name: flux-system\n# ICI vous retrouverez l'adresse de votre repo GitLab    \n  url: https://gitlab.com/jouve.thomas/kub-workshop-snowcamp-2024.git\n</code></pre>"},{"location":"flux/#notre-premiere-recette","title":"Notre premi\u00e8re recette","text":"<p>On va pouvoir lui dire d'appliquer la configuration grace aux objets Kustomization.</p> <p>Si on regarde par example le fichier <code>flux/repo.yaml</code> ./flux/repo.yaml<pre><code>\n</code></pre></p> <p>On peut voir que l'on d\u00e9crit en language Flux un nouveau r\u00e9pertoire \u00e0 surveiller <code>./flux/repository</code> dans notre <code>GitRepository:flux-system</code>.</p>"},{"location":"flux/#definition-des-helmrepository","title":"D\u00e9finition des HelmRepository","text":"<p>Dans ce r\u00e9pertoire <code>./flux/repository</code> on retrouve par example le fichier <code>nginx.yaml</code> : ./flux/repository/nginx.yaml<pre><code>\n</code></pre></p> <p>Cette fois-ci on configure un HelmRepository qui va permettre \u00e0 Flux de r\u00e9cup\u00e9rer les informations sur les charts disponibles dans le repo Helm.</p> <p>On pourra faire r\u00e9f\u00e9rence \u00e0 ce chart sous le nom <code>nginx-ingress-controller</code>.</p>"},{"location":"flux/#nouveaux-commis","title":"Nouveaux commis","text":"<p>On va maintenant deplacer ce fichier <code>repo.yaml</code> dans le r\u00e9pertoire <code>flux/devoxx-cluster/</code> qui est le seul, pour le moment, que connait Flux.</p> <p>Effectivement le fichier <code>gotk-sync.yaml</code> indique que seul le r\u00e9pertoire <code>./flux/devoxx-cluster</code> est scrupt\u00e9 par Flux :</p> ./flux/devoxx-cluster/flux-system/gotk-sync.yaml<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 10m0s\n  path: ./flux/devoxx-cluster\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n</code></pre>"},{"location":"flux/#on-envoie-les-commandes-en-cuisine","title":"On envoie les commandes en cuisine","text":"<p>Et on <code>push commit</code>, car maintenant c'est Flux qui se charge de faire la synchronisation sur le cluster depuis notre repo.</p> <pre><code>cp flux/repo.yaml flux/devoxx-cluster/repo.yaml\ngit add flux/devoxx-cluster/repo.yaml\ngit commit -am \":satellite_orbital: Setup Helm repos\" &amp;&amp; git push\n</code></pre> <p>On peut observer la r\u00e9conciliation avec la commande suivante :</p> <pre><code>flux get kustomizations --watch\n</code></pre> <p>On a quelque chose comme \u00e7a :</p> <ul> <li>il y a 2 r\u00e9pertoires \u00e0 surveiller (2 <code>Kustomization</code>)</li> <li>la synchronisation est active (<code>SUSPENDED : False</code>)</li> <li>et \u00e0 jour (<code>READY : True</code>)</li> </ul> <p>Il indique aussi quelle est la r\u00e9vision utilis\u00e9e pour la synchronisation (ici :<code>main@sha1:c80d7d4c</code>).</p> <pre><code>NAME            REVISION                SUSPENDED       READY   MESSAGE                              \nflux-system     main@sha1:c80d7d4c      False           True    Applied revision: main@sha1:c80d7d4c\nrepos           main@sha1:c80d7d4c      False           True    Applied revision: main@sha1:c80d7d4c\n</code></pre> <p>On peux v\u00e9rifier en regardant si il a bien cr\u00e9\u00e9 nos resources <code>HelmRepository</code> :</p> <pre><code>kubectl get HelmRepository -A\n</code></pre> <pre><code>NAMESPACE     NAME                       URL                                          AGE    READY   STATUS\nflux-system   cert-manager               https://charts.jetstack.io                   112s   True    stored artifact: revision 'sha256:c930db5052b76d7be3026686612fa09f89a23f8547a8ecad7496d788e34964e5'\nflux-system   external-secrets           https://charts.external-secrets.io           112s   True    stored artifact: revision 'sha256:35fa1d6332232e3c6d032627547ffc74c7e61c4729ed1daa680b2202c61a78da'\nflux-system   nginx-ingress-controller   https://kubernetes.github.io/ingress-nginx   112s   True    stored artifact: revision 'sha256:e6a6c9e8f3682deea82b3bc22506d4fdabd667ce37cb1d0f7509459ca92c3426'\n</code></pre>"},{"location":"flux/#ingress-controller","title":"Ingress Controller","text":"<p>Tout est pr\u00eat dans le r\u00e9pertoire <code>./nginx-ingress-controller/flux</code> pour d\u00e9ployer notre Ingress Controller.</p>"},{"location":"flux/#le-descriptif-de-notre-recette-est-une-kustomization","title":"Le descriptif de notre recette est une <code>Kustomization</code>","text":"<p>Ici ce n'est pas une <code>Kustomization</code> mais une <code>Kustomization</code> \ud83e\uddcc</p> <p>Il faut lire :</p> <ul> <li>Kustomization@kustomize.toolkit.fluxcd.io/v1 est la CRD de FLux pour les objects Flux (le liens vers un repo / repertoire / interval de scrapping)</li> <li>Kustomization@kustomize.config.k8s.io/v1beta1 est l'objet Kustomize de Kubernetes</li> </ul> <p>Ici on d\u00e9clare une recette avec le nom <code>flux-nginx-ingress-controller</code> et qu'il est n\u00e9cessaire d'utiliser les fichiers <code>nginx-ingress-controller.yaml</code> et <code>ns.yaml</code> pour d\u00e9ployer notre Ingress Controller.</p> ./nginx-ingress-controller/flux/ks.yaml<pre><code>\n</code></pre>"},{"location":"flux/#la-definition-de-notre-namespace","title":"La definition de notre Namespace","text":"<p>Avec un simple fichier manifest vanilla :</p> ./nginx-ingress-controller/flux/ns.yaml"},{"location":"flux/#et-la-definition-de-notre-helmrelease","title":"Et la definition de notre HelmRelease","text":"./nginx-ingress-controller/flux/nginx-ingress-controller.yml"},{"location":"flux/#chaud-devant","title":"Chaud devant !","text":"<p>Avant de lancer la commande en cuisine, soyons fou et supprimons notre Ingress Controller pr\u00e9c\u00e9demment install\u00e9 pour laisser faire Flux.</p> <p>Ce n'est pas obligatoire, mais c'est pour voir la magie de Flux.</p> <p>En fait Flux va juste r\u00e9appliquer la configuration, donc si vous ne supprimez pas l'Ingress Controller, il va juste le mettre \u00e0 jour. Il utilisera <code>install</code> ou <code>upgrade</code> en fonction de l'\u00e9tat de l'objet HelmRelease.   </p> <pre><code> helm list -A\n helm uninstall ingress-nginx -n nginx-ingress-controller\n helm list -A\n kubectl get all -n nginx-ingress-controller\n kubectl delete ns nginx-ingress-controller\n kubectl get ns\n</code></pre>"},{"location":"flux/#on-envoie-la-sauce","title":"On envoie la sauce","text":"<p>On va maintenant d\u00e9placer le fichier <code>flux/nginx-ingress-controller.yaml</code> dans le r\u00e9pertoire <code>flux/devoxx-cluster/</code> comme pour les repos Helm.</p> ./flux/nginx-ingress-controller.yaml<pre><code>\n</code></pre> <p><pre><code>cp flux/nginx-ingress-controller.yaml flux/devoxx-cluster/nginx-ingress-controller.yaml\ngit add flux/devoxx-cluster/nginx-ingress-controller.yaml\ngit commit -am \":satellite_orbital: Setup Ingress Controller\" &amp;&amp; git push\n</code></pre> On observe la synchro avec la commande suivante :</p> <pre><code>flux get kustomizations --watch\n</code></pre> <pre><code> helm list -A\n echo \"Helm list ne nous retourne rien car c'est Flux qui g\u00e9re maintenant\"\n sleep 20\n echo \"On utilise : \"\n kubectl get HelmRepository -A\n kubectl get HelmChart -A\n kubectl get HelmRelease -A\n kubectl get ns\n kubectl get all -n nginx-ingress-controller\n</code></pre>"},{"location":"flux/#external-secrets","title":"External Secrets","text":"<p>On va maintenant d\u00e9ployer notre External Secrets. Comme \u00e7a pas besoin de stocker les secrets \u00e0 la main.</p>"},{"location":"flux/#setup-des-secrets","title":"Setup des secrets","text":"<p>Commen\u00e7ons par cr\u00e9er les variables d'environnement dans notre repo GitLab. (Ils ne sont pas repris lors du fork et heureusement ...).</p> <ul> <li>Soit on passe par l'UI de Gitlab</li> </ul> <p></p> <ul> <li>Soit on utilise l'API de Gitlab</li> </ul> <p>Il vous faut l'ID du projet, vous pouvez le trouver ici :</p> <p></p> <pre><code>PROJECT_ID= &lt;YOUR_PROJECT_ID&gt;\n</code></pre> <pre><code>curl --request POST --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n     \"https://gitlab.com/api/v4/projects/${PROJECT_ID}/variables\" \\\n     --form \"key=API_MAIL\" --form \"value=${API_MAIL}\"\n\ncurl --request POST --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n     \"https://gitlab.com/api/v4/projects/${PROJECT_ID}/variables\" \\\n     --form \"key=API_KEY\" --form \"value=${API_KEY}\"\n</code></pre> <p>Il ne nous reste plus qu'\u00e0 :</p> <ul> <li>cr\u00e9er notre <code>Secret</code> dans notre cluster Kubernetes comme vu pr\u00e9c\u00e9dement, mais cette fois avec votre compte.</li> <li>modifier le fichier <code>./external-secrets/flux/external-secrets-secret-store.yml</code> pour y mettre votre ID project.</li> </ul> <p>./external-secrets/flux/external-secrets-secret-store.yml<pre><code>\n</code></pre> Par exemple avec :  <pre><code>sed -i \"s/&lt;PROJECT_ID&gt;/$PROJECT_ID/\" ./external-secrets/flux/external-secrets-secret-store.yml\n</code></pre></p>"},{"location":"flux/#presentation-de-notre-kustomization","title":"Pr\u00e9sentation de notre kustomization","text":"./external-secrets/flux/ks.yml <ul> <li>Cr\u00e9ation du NS</li> <li>D\u00e9ployement des CRDS (Custom Resource Definition)</li> <li>D\u00e9ployement de l'op\u00e9rateur</li> <li>D\u00e9ployement des CRS (Custom Resource) les diff\u00e9rents providers (ici GitLab)</li> <li>La d\u00e9finition de notre secret store</li> </ul>"},{"location":"flux/#chaud-devant_1","title":"Chaud devant !","text":"<p>Fin pr\u00eat pour lancer la commande : <pre><code>cp flux/external-secrets.yaml flux/devoxx-cluster/external-secrets.yaml\ngit add flux/devoxx-cluster/external-secrets.yaml\ngit commit -am \":satellite_orbital: Setup External Secret\" &amp;&amp; git push\n</code></pre></p>"},{"location":"flux/#external-dns-cert-manager","title":"External DNS &amp;&amp; Cert-manager","text":"<p>Un dernier petit tips pour la route :</p> <p>./flux/cert-manager.yaml La commande <code>dependsOn</code> permet de d\u00e9finir une d\u00e9pendance entre les diff\u00e9rents objets Flux. <pre><code>flowchart TD\n  A[Cert Manager] --&gt;|dependsOn| B[external-dns];\n  A[Cert Manager] --&gt;|dependsOn| C[nginx-ingress-controller];\n  B --&gt;|dependsOn| D[external-secrets];</code></pre></p> <p>On s'assure ainsi que les diff\u00e9rentes recettes sont appliqu\u00e9es dans l'ordre.</p> <p>Retournons \u00e0 la recette pour encore plus de d\u00e9couvertes \u27a1\ufe0f</p>"},{"location":"gitlab/","title":"Les GitLab environnements","text":""},{"location":"gitlab/#pre-requis","title":"Pr\u00e9-requis","text":"<p>Avant de d\u00e9marrer cette partie, il est n\u00e9cessaire de forker le projet car vous aurez besoin d'\u00eatre Owner du projet pour faire les manipulations. Pour cela, il suffit de forker le projet.</p> <p>Pas besoin de forker 2 fois</p> <p>Si vous avez fait la partie flux avant celle-ci et que vous avez d\u00e9j\u00e0 fork\u00e9 le projet, pas besoin de faire un nouveau fork. Vous pouvez r\u00e9utiliser le 1er.</p> <p>Gitpod</p> <p>Si vous utilisez Gitpod n'oubliez pas de t\u00e9l\u00e9charger votre <code>kubeconfig</code> alias <code>cluster-ovh-${TF_VAR_OVH_CLOUD_PROJECT_KUBE_NAME}.yml</code>. Avec le fork vous allez d\u00e9marrer avec un nouveau pod et par cons\u00e9quence vos fichiers locaux ne seront plus accessibles.</p>"},{"location":"gitlab/#installation","title":"Installation","text":"<p>Pour installer l'agent, il faut suivre la doc GitLab qui consiste \u00e0 :</p> <ul> <li>ajouter le repo Helm pour GitLab</li> </ul> <pre><code>helm repo add gitlab https://charts.gitlab.io\nhelm repo update\n</code></pre> <ul> <li>Et installer l'agent</li> </ul> <p>Un exemple mais \u00e0 r\u00e9cup\u00e9rer depuis l'IHM GitLab \u00e0 cause du token</p> <pre><code>helm upgrade --install demo-gitlab-env gitlab/gitlab-agent \\\n    --namespace gitlab-agent-demo-gitlab-env \\\n    --create-namespace \\\n    --set image.tag=v16.8.0-rc2 \\ # (1)\n    --set config.token=glagent-v... \\ # (2)\n    --set config.kasAddress=wss://kas.gitlab.com\n</code></pre> <ol> <li>\ud83d\udd22 La version peut \u00eatre diff\u00e9rente</li> <li>\ud83d\udd11 Ce token est g\u00e9n\u00e9r\u00e9 par GitLab au moment d'enregistrer l'agent.</li> </ol> <p>L'agent est install\u00e9 correctement</p> <pre><code>Thank you for installing gitlab-agent.\n\nYour release is named demo-gitlab-env.    \n</code></pre> <p>On v\u00e9rifie que tout est ok</p> <p><pre><code>kubectl get po -n &lt;namespace&gt; # (1)\n</code></pre> 1. Namespace can be different</p> <p>Les pods gitlab-agent sont d\u00e9marr\u00e9s</p> <pre><code>NAME                                              READY   STATUS    RESTARTS   AGE\ndemo-gitlab-env-gitlab-agent-v1-8b8bc9c85-2tq2b   1/1     Running   0          2m48s\ndemo-gitlab-env-gitlab-agent-v1-8b8bc9c85-w8wm7   1/1     Running   0          2m48s\n</code></pre>"},{"location":"gitlab/#utilisation","title":"Utilisation","text":"<p>Gr\u00e2ce \u00e0 l'agent on va pouvoir int\u00e9ragir depuis GitLab-CI sans avoir besoin d'un <code>kubeconfig</code></p> <p>Par exemple, on peut d\u00e9ployer le helm custom disponible dans le r\u00e9pertoire <code>demo-gitlab</code></p> <p>On cr\u00e9e un job dans le fichier <code>.gitlab-ci.yml</code> \u00e0 la racine du projet.</p> <p>Supprimer le job existant</p> <p>Vous devez supprimer le contenu du <code>.gitlab-ci.yml</code> existant avant d'ajouter les \u00e9l\u00e9ments</p> <p><pre><code>stages:\n  - \ud83c\udfd7\ufe0f # (1)\n\n\ud83d\udea7_deploy_env:\n  stage: \ud83c\udfd7\ufe0f\n  image:\n    name: dtzar/helm-kubectl\n  script:\n    - kubectl config use-context yodamad-workshops/2024/devoxx/kube-workshop:demo-gitlab-env # (2)\n    - helm upgrade ${CI_COMMIT_REF_SLUG}-env ./demo-gitlab/ --set labName=${CI_PROJECT_NAME}-${CI_COMMIT_REF_SLUG} --install --create-namespace -n ${CI_PROJECT_NAME}-${CI_COMMIT_REF_SLUG}\n  environment:\n    name: ${CI_COMMIT_REF_SLUG}\n    url : https://${CI_PROJECT_NAME}-${CI_COMMIT_REF_SLUG}.&lt;votre_trigramme&gt;.grunty.uk # (3)\n  needs: []\n</code></pre> 1. \u2622\ufe0f Ne pas oublier d'ajouter le stage du job dans la liste des stages 2. \ud83d\udef0\ufe0f Ne pas oublier de changer le path 3. \ud83d\udd22 Ne pas oublier de mettre votre trigramme</p> <p>Comprendre l'exemple</p> <p>Dans cet exemple, on a plusieurs choses:</p> <ul> <li><code>kubectl config use-context</code> pour connecter la CI au cluster</li> <li><code>yodamad-workshops/2024/devoxx/kube-workshop</code> correspond au path vers votre projet fork\u00e9 sur gitlab.com. A adapter en fonction de vos donn\u00e9es</li> <li><code>demo-gitlab-env</code> correspond au nom du projet. A adapter en fonction de vos donn\u00e9es</li> <li><code>environment</code> : d\u00e9crit l'environnement d\u00e9ploy\u00e9 en lien avec ce job<ul> <li><code>url</code> : une URL dynamique qui est construit en fonction du nom du projet et de la branche</li> <li><code>on_stop</code> : l'action \u00e0 d\u00e9clencher quand l'environnement est arr\u00eat\u00e9, par exemple quand la branche associ\u00e9e est supprim\u00e9e</li> </ul> </li> </ul> <p>Quel context utiliser</p> <p>Si vous peinez \u00e0 trouver quel est le bon nom du context \u00e0 utiliser pour la partie <code>kubectl config use-context</code>, vous pouvez ajouter dans le job l'instruction suivante : <code>kubectl config get-contexts</code> qui vous listera les contextes disponibles.</p> <pre><code>\ud83d\udea7_deploy_env:\n  [...]\n    script:\n      - kubectl config get-contexts\n</code></pre> <p>Gr\u00e2ce \u00e0 ce que l'on a mis en place pr\u00e9c\u00e9demment, on pourra simplement avoir un nouvel environnement avec une URL reconnnue (merci <code>external-dns</code>), s\u00e9curis\u00e9e (merci <code>cert-manager</code>) pour que les devs et les testeurs du projet puissent acc\u00e9der \u00e0 la version souhait\u00e9e.</p> <p>Mettre votre trigramme dans l'ingress</p> <p>Dans le fichier <code>demo-gitlab/values.yaml</code>, il faut changer <code>&lt;votre_trigramme&gt;</code> par votre trigramme et commiter</p> <p>On peut commiter le fichier <code>.gitlab-ci.yml</code>, et une fois que le pipeline est termin\u00e9 (avec succ\u00e8s), on peut voir que tout est bien cr\u00e9\u00e9</p> <pre><code>kubectl get po -n gitlab-agent-demo-gitlab-env\n</code></pre> <p>Tout est d\u00e9ploy\u00e9</p> <pre><code>NAME                                                  READY   STATUS    RESTARTS   AGE\npod/demo-gitlab-env-gitlab-agent-v1-8b8bc9c85-2tq2b   1/1     Running   0          11h\npod/demo-gitlab-env-gitlab-agent-v1-8b8bc9c85-w8wm7   1/1     Running   0          11h\n\nNAME                                              READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/demo-gitlab-env-gitlab-agent-v1   2/2     2            2           11h\n\nNAME                                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/demo-gitlab-env-gitlab-agent-v1-8b8bc9c85   2         2         2       11h\n</code></pre> <p>Pour continuer, cr\u00e9er une nouvelle branche et changer l'image par d\u00e9faut dans <code>demo-gitlab/values.yml</code>:</p> <p>Avant:</p> <pre><code>  image: registry.gitlab.com/yodamad-workshops/kub-workshop/nginxdemos\n  version: plain-text\n</code></pre> <p>Apr\u00e8s:</p> <pre><code>  image: registry.gitlab.com/yodamad-workshops/kub-workshop/asciinematic\n  version: latest\n</code></pre> <p>Commiter sur la nouvelle branche et attendre que le pipeline se termine.</p> <p>Dans le menu <code>Operate &gt; Environments</code>, il y a 2 environnements disponibles:</p> <p></p> <p>En ouvrant les 2 environnements (via <code>Open</code>), on a bien : </p> <ul> <li>une IHM simple sur <code>main</code></li> <li>un cinema ascii sur la nouvelle branche</li> </ul>"},{"location":"gitlab/#nettoyage","title":"Nettoyage","text":"<p>Il ne faut pas oublier de nettoyer ses environements lorsqu'ils ne sont plus n\u00e9cessaires (\ud83c\udf31 la plan\u00e8te vous dira merci).</p> <p>Pour cela, on impl\u00e9mente le job qui est r\u00e9f\u00e9renc\u00e9 dans le <code>on_stop</code></p> <pre><code>stages:\n  [...]\n  - \ud83c\udfd7\ufe0f\n  - \ud83e\uddf9 # (1)\n\n\ud83e\uddfc_clean:\n  stage: \ud83e\uddf9\n  image:\n    name: dtzar/helm-kubectl\n  script:\n    - kubectl config use-context yodamad-workshops/kub-workshop:demo-gitlab-env\n    - helm uninstall ${CI_COMMIT_REF_SLUG}-env -n ${CI_PROJECT_NAME}-${CI_COMMIT_REF_SLUG}\n    - kubectl delete ns ${CI_PROJECT_NAME}-${CI_COMMIT_REF_SLUG}\n  when: manual\n  environment:\n    name: ${CI_COMMIT_REF_SLUG}\n    action: stop\n</code></pre> <p>Il faut \u00e9galement ajouter l'instruction <code>on_stop</code> au job de d\u00e9ploiement :</p> <pre><code>\ud83d\udea7_deploy_env:\n  [...]\n  environment:\n    [...]\n    on_stop: \ud83e\uddfc_clean\n</code></pre> <ol> <li>\u2622\ufe0f Ne pas oublier d'ajouter le stage du job dans la liste des stages</li> </ol> <p>Arr\u00eater l'environnement depuis l'IHM GitLab (via le menu <code>Operate &gt; Environments</code>)</p> <p>L'environnement est bien supprim\u00e9</p> <ul> <li>Le job <code>on_stop</code> est bien d\u00e9clench\u00e9</li> </ul> <p></p> <ul> <li>Dans <code>Operate &gt; Environments</code>, il n'y a plus qu'un environnement</li> <li>Dans le cluster, les \u00e9lements ont bien \u00e9t\u00e9 supprim\u00e9ss</li> </ul> <pre><code>kubectl get ns kub-workshop\n</code></pre> <pre><code>Error from server (NotFound): namespaces \"kub-workshop-snowcamp-2024-demo\" not found\n</code></pre> <p>Un premier usage plut\u00f4t pratique !</p> <p>Retournons \u00e0 la recette pour encore plus de d\u00e9couvertes \u27a1\ufe0f</p>"},{"location":"initial-stage/","title":"Un premier d\u00e9ploiement dans notre cluster","text":"<p>Et si on essayait de d\u00e9ployer un truc dans notre cluster, par exemple un cinema en ascii \ud83c\udf7f</p> <p>Note</p> <p>Rebasculer \u00e0 la racine du repo <code>cd ..</code></p> <pre><code>kubectl apply -f demos/simple-deployment.yml\n</code></pre> Pour les curieux, le contenu de <code>simple-deployment.yml</code> <pre><code>\n</code></pre> <p>Installation r\u00e9ussie</p> <pre><code>namespace/demos created\ndeployment.apps/simple-deployment created\nservice/simple-deployment-service created\n</code></pre> <p>On v\u00e9rifie que tout est ok:</p> <pre><code>kubectl get deployments -n demos\n</code></pre> <p>D\u00e9mo d\u00e9ploy\u00e9e</p> <pre><code>NAME                READY   UP-TO-DATE   AVAILABLE   AGE\nsimple-deployment   1/1     1            1           39s\n</code></pre> <p>Mainteant, pour acc\u00e9der \u00e0 notre cin\u00e9ma, il faut cr\u00e9er un tunnel vers notre cluster:</p> <pre><code>kubectl -n demos port-forward $(kubectl get pods -o=name -n demos) 8080:80\n</code></pre> <p>\ud83c\udfa5 Notre application est disponible http://localhost:8080</p> <p>Ok, ca marche mais c'est pas super pratique pour:</p> <ul> <li>travailler en \u00e9quipe</li> <li>il faut faire le mapping des ports et des hosts pour chaque application</li> <li>...</li> </ul> <p>Pour acc\u00e9der depuis l'ext\u00e9rieur, il va nous falloir un soup\u00e7on d'Ingress et un Ingress Controller pour g\u00e9rer cela pour nous.</p> <p>\ud83d\udee3\ufe0f En route pour la d\u00e9couverte de comment Nginx controller va nous faciliter la vie \u27a1\ufe0f</p>"},{"location":"kube-downscaler/","title":"Repos du guerrier","text":"<p>Vu qu'on a bien travaill\u00e9, on peut se reposer un peu et nos pods aussi, on va donc mettre en place kube-downscaler pour \u00e9teindre nos workloads lors des temps de pause.</p>"},{"location":"kube-downscaler/#installation","title":"Installation","text":"<p><code>kube-downscaler</code> est un projet plus r\u00e9cent, il n'est pas encore tr\u00e8s industrialis\u00e9 donc il est n\u00e9cessaire de faire quelques adaptations pour le d\u00e9ployer:</p> <ul> <li>changer le namespace <code>default</code> en <code>kube-downscaler</code> dans <code>deploy/rbac.yml</code></li> <li>d\u00e9sactiver le mode <code>dry-run</code> dans <code>deploy/deployment.yml</code> </li> <li>(pour la d\u00e9mo) r\u00e9duire la grace period de prise en compte des composants \u00e0 0min (plut\u00f4t que 15 par d\u00e9faut) dans <code>deploy/deployment.yml</code></li> <li>(pour la d\u00e9mo) appliquer uniquement les policies sur le namespace <code>demos</code> dans <code>deploy/deployment.yml</code></li> </ul> <pre><code>    - --grace-period=0\n    - --namespace=demos\n</code></pre> Pour les curieux, les fichiers <code>rbac.yml</code> et <code>deployment.yml</code> <p><code>rbac.yml</code>: <pre><code>\n</code></pre></p> <p><code>deployment.yml</code>: <pre><code>\n</code></pre></p> <p>Les fichiers sont dans le r\u00e9pertoire <code>kube-downscaler/deploy</code>, on utilise kustomize pour d\u00e9ployer (contrairement \u00e0 Helm les autres fois).</p> <pre><code>kubectl apply -k kube-downscaler/deploy/ # (1)\n</code></pre> <ol> <li>On utilise <code>-k</code> au lieu du <code>-f</code> habituel</li> </ol> <p>Tout est cr\u00e9\u00e9</p> <pre><code>namespace/kube-downscaler created\nserviceaccount/kube-downscaler created\nclusterrole.rbac.authorization.k8s.io/kube-downscaler created\nclusterrolebinding.rbac.authorization.k8s.io/kube-downscaler created\nconfigmap/kube-downscaler created\ndeployment.apps/kube-downscaler created    \n</code></pre> <p>On v\u00e9rifie que tout est ok</p> <pre><code>kubectl get all -n kube-downscaler\n</code></pre> <p>Tout est op\u00e9rationnel</p> <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE\npod/kube-downscaler-f5cbb6cfc-f2j9w   1/1     Running   0          39s\n\nNAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kube-downscaler   1/1     1            1           39s\n\nNAME                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kube-downscaler-f5cbb6cfc   1         1         1       39s    \n</code></pre>"},{"location":"kube-downscaler/#test","title":"Test","text":"<p>Vu qu'on aime pas trop bosser, on va configurer <code>kube-downscaler</code> pour que nos pods ne soient d\u00e9marr\u00e9s que le lundi de 7h30 \u00e0 18h \ud83e\udee2</p> <pre><code>  annotations:\n    downscaler/uptime: Mon-Mon 07:30-18:00 CET\n    downscaler/force-downtime: \"true\"\n</code></pre> <p>Pour cela, on annote notre namespace <code>demos</code> pour qu'il ne soit up que sur cette p\u00e9riode.</p> <p>NB: <code>kube-dowscaler</code> peut \u00eatre configur\u00e9 \u00e0 diff\u00e9rents niveaux, on aurait pu annoter les deployments directement plut\u00f4t que le namespace.</p> <pre><code>kubectl annotate ns demos 'downscaler/uptime=Mon-Mon 07:30-18:00 CET'\n</code></pre> <p>Le namespace est annot\u00e9 pour s'\u00e9teindre</p> <p><pre><code>kubectl describe ns demos\n</code></pre> <pre><code>Name:         demos\nLabels:       kubernetes.io/metadata.name=demos\nAnnotations:  downscaler/uptime: Mon-Fri 21:30-23:30 CET\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.    \n</code></pre></p> <p>On peut voir imm\u00e9diatement que nos pods sont \u00e9teints et que nos d\u00e9ploiements sont \u00e0 0</p> <p>Tout est \u00e9teint</p> <p><pre><code>kubectl get po -n demos\n</code></pre> <pre><code>No resources found in demos namespace\n</code></pre></p> <p>et les d\u00e9ploiements <pre><code>kubectl get deploy -n demos\n</code></pre> <pre><code>NAME                    READY   UP-TO-DATE   AVAILABLE   AGE\nasciinematic-hardened   0/0     0            0           156m\nnew-deployment          0/0     0            0           28h\nsimple-deployment       0/0     0            0           28h    \n</code></pre></p> <p>On peut voir dans les logs de <code>kube-downscaler</code> qu'il a trait\u00e9 notre configuration</p> <pre><code>kubectl logs -f $(kubectl get po -n kube-downscaler | grep kube-downscaler | cut -d' ' -f1) -n kube-downscaler\n</code></pre> <p>Info</p> <pre><code>2023-12-20 18:28:55,501 INFO: Downscaler v23.2.0 started with debug=False, default_downtime=never, default_uptime=Mon-Fri 07:30-20:30 CET, deployment_time_annotation=None, downscale_period=never, downtime_replicas=0, dry_run=False, enable_events=False, exclude_deployments=kube-downscaler,downscaler, exclude_namespaces=kube-system, grace_period=0, include_resources=deployments, interval=2, namespace=demos, once=False, upscale_period=never\n2023-12-20 18:47:29,649 INFO: Scaling down Deployment demos/asciinematic-hardened from 1 to 0 replicas (uptime: Mon-Fri 21:30-23:30 CET, downtime: never)\n2023-12-20 18:47:29,675 INFO: Scaling down Deployment demos/new-deployment from 1 to 0 replicas (uptime: Mon-Fri 21:30-23:30 CET, downtime: never)\n2023-12-20 18:47:29,700 INFO: Scaling down Deployment demos/simple-deployment from 1 to 0 replicas (uptime: Mon-Fri 21:30-23:30 CET, downtime: never)\n</code></pre> <p>Voil\u00e0, on a une recette bien compl\u00e8te, faisons un petit r\u00e9capitulatif \u27a1\ufe0f</p>"},{"location":"kyverno/","title":"Set up Kyverno","text":"<p>Pour contr\u00f4ler ce qu'il se passe dans notre cluster, nous allons utiliser kyverno.</p>"},{"location":"kyverno/#installation","title":"Installation","text":"<p>Kyverno fournit un Helm chart pour simplifer l'installation.</p> <pre><code>helm repo add kyverno https://kyverno.github.io/kyverno/\nhelm repo update\n</code></pre> <p>Une fois configur\u00e9, on peut installer le Helm chart:</p> <pre><code>helm install kyverno kyverno/kyverno -n kyverno --create-namespace\n</code></pre> <p>Success</p> <pre><code>Thank you for installing kyverno! Your release is named kyverno.\n\nThe following components have been installed in your cluster:\n- CRDs\n- Admission controller\n- Reports controller\n- Cleanup controller\n- Background controller\n</code></pre> <p>On v\u00e9rifie que tout est ok cot\u00e9 namespace <code>kyverno</code></p> <pre><code>kubectl get all -n kyverno\n</code></pre> Les composants install\u00e9s par kyverno <pre><code>NAME                                                READY   STATUS    RESTARTS   AGE\npod/kyverno-admission-controller-547894dbc9-srg54   1/1     Running   0          2m43s\npod/kyverno-background-controller-696f7765d-d8knf   1/1     Running   0          2m43s\npod/kyverno-cleanup-controller-567bb6695c-mxf7s     1/1     Running   0          2m43s\npod/kyverno-reports-controller-5799587486-x2gjp     1/1     Running   0          2m43s\n\nNAME                                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nservice/kyverno-background-controller-metrics   ClusterIP   10.3.236.41    &lt;none&gt;        8000/TCP   2m45s\nservice/kyverno-cleanup-controller              ClusterIP   10.3.230.129   &lt;none&gt;        443/TCP    2m45s\nservice/kyverno-cleanup-controller-metrics      ClusterIP   10.3.189.211   &lt;none&gt;        8000/TCP   2m45s\nservice/kyverno-reports-controller-metrics      ClusterIP   10.3.153.42    &lt;none&gt;        8000/TCP   2m45s\nservice/kyverno-svc                             ClusterIP   10.3.165.95    &lt;none&gt;        443/TCP    2m45s\nservice/kyverno-svc-metrics                     ClusterIP   10.3.106.166   &lt;none&gt;        8000/TCP   2m45s\n\nNAME                                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kyverno-admission-controller    1/1     1            1           2m44s\ndeployment.apps/kyverno-background-controller   1/1     1            1           2m44s\ndeployment.apps/kyverno-cleanup-controller      1/1     1            1           2m44s\ndeployment.apps/kyverno-reports-controller      1/1     1            1           2m44s\n\nNAME                                                      DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kyverno-admission-controller-547894dbc9   1         1         1       2m44s\nreplicaset.apps/kyverno-background-controller-696f7765d   1         1         1       2m44s\nreplicaset.apps/kyverno-cleanup-controller-567bb6695c     1         1         1       2m44s\nreplicaset.apps/kyverno-reports-controller-5799587486     1         1         1       2m44s\n\nNAME                                                      SCHEDULE       SUSPEND   ACTIVE   LAST SCHEDULE   AGE\ncronjob.batch/kyverno-cleanup-admission-reports           */10 * * * *   False     0        &lt;none&gt;          2m44s\ncronjob.batch/kyverno-cleanup-cluster-admission-reports   */10 * * * *   False     0        &lt;none&gt;          2m44s\n</code></pre>"},{"location":"kyverno/#ma-premiere-policy","title":"Ma premi\u00e8re policy","text":"<p>Comme nous l'a demand\u00e9 le chef de brigade, on veut contr\u00f4ler nos ingr\u00e9dients : mettons en place une policy pour limiter l'usage d'images Docker provenant uniquement d'une registry priv\u00e9e en laquelle on a confiance, dans notre demo, le registry de gitlab.com de notre projet.</p> <p>D\u00e9finissons notre policy <code>ClusterPolicy</code> qui sera \u00e0 l'\u00e9chelle du cluster complet (les \u2795 vous donnent des indications pour mieux comprendre)</p> <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  annotations:\n    policies.kyverno.io/title: Restrict Image Registries\n    policies.kyverno.io/description: &gt;-\n      Only images from specific gitlab.com registry are allowed.\nspec:\n  validationFailureAction: Enforce # (1)\n  background: true\n  rules:\n  - name: validate-registries\n    match:\n      any:  # (2)\n      - resources:\n          kinds:\n          - Pod\n      exclude: # (3)\n        any:\n          - resources:\n              namespaces:\n                - kube-system\n                - external-dns\n                - external-secrets\n                - nginx-ingress-controller\n                - kyverno\n                - cert-manager \n    validate:  # (4)\n      message: \"Unauthorized registry.\"\n      pattern:\n        spec:  # (5)\n          =(ephemeralContainers):\n          - image: \"registry.gitlab.com/*\"\n          =(initContainers):\n          - image: \"registry.gitlab.com/*\"\n          containers:\n          - image: \"registry.gitlab.com/*\"\n</code></pre> <ol> <li>\ud83d\udcaa On utilise une action de type <code>Enforce</code> qui est bloquante. Il existe aussi le type <code>Audit</code> qui est un warning mais non bloquant</li> <li>\ud83d\udd0d On applique \u00e0 tous les Pod</li> <li>\ud83d\udeb8 On filtre les namespaces que l'on a d\u00e9j\u00e0 instanci\u00e9 et qui sont des namespaces d'administration</li> <li>\ud83d\udc7d Le type de r\u00e8gle, ici <code>validate</code></li> <li>\ud83d\udcdd On sp\u00e9cifie pour chaque type, les origines autoris\u00e9es</li> </ol> <p>On peut d\u00e9ployer notre policy:</p> <pre><code>kubectl apply -f kyverno/kyverno-registry-policy.yml\n</code></pre> <p>et on v\u00e9rifie qu'elle est bien op\u00e9rationnelle</p> <pre><code>kubectl get clusterpolicies.kyverno.io -n kyverno\n</code></pre> <p>Policy au statut <code>Ready</code></p> <pre><code>NAME                      ADMISSION   BACKGROUND   VALIDATE ACTION   READY   AGE   MESSAGE\nrestrict-image-registry   true        true         Enforce           True    18m   Ready\n</code></pre>"},{"location":"kyverno/#verification","title":"V\u00e9rification","text":"<p>Essayons de d\u00e9marrer un pod avec une image issue de Docker hub:</p> <pre><code>kubectl run demo-nginx --image=nginx:latest -n demos\n</code></pre> <p>Impossible de d\u00e9marrer le pod</p> <pre><code>Error from server: admission webhook \"validate.kyverno.svc-fail\" denied the request:\n\nresource Pod/demos/demo-nginx was blocked due to the following policies\n\nrestrict-image-registry:\n  validate-registries: 'validation error: Unauthorized registry. rule validate-registries\n    failed at path /spec/containers/0/image/'\n</code></pre> <p>On met \u00e0 jour notre <code>Deployment</code> pour utiliser une image issue de la registry GitLab:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\n# ...\nspec:\n  selector:\n    matchLabels:\n      app: nginx-hardened\n  template:\n    metadata:\n      labels:\n        app: nginx-hardened\n    spec:\n      containers:\n        - image: registry.gitlab.com/yodamad-workshops/kub-workshop/asciinematic:latest # (1)\n          name: asciinematic-hardened\n          ports:\n            - containerPort: 80\n      imagePullSecrets:\n        - name: gitlabcred # (2)\n</code></pre> <ol> <li>\ud83d\udc33 On utilise une image de notre registry priv\u00e9e</li> <li>\ud83d\udd10 C'est une registry priv\u00e9e, il faut s'authentifier...</li> </ol> <p>On voit qu'il faut ajout un secret pour \u00eatre capable de <code>pull</code> une image depuis une registry priv\u00e9e, normal... Mais du coup, il faut cr\u00e9er un secret pour cela. Alors on va en cr\u00e9er un</p> <pre><code>kubectl create secret gitlabcred regcred --docker-server=&lt;your-registry-server&gt; --docker-username=&lt;your-name&gt; --docker-password=&lt;your-pword&gt; --docker-email=&lt;your-email&gt;\n</code></pre> <p>Le chef de la brigade de la s\u00e9curit\u00e9</p> <p>Pas de secret en dur dans mon cluster !! \ud83e\udd2c</p> <p>Oups, il a pas tort \ud83d\ude05 On devrait plut\u00f4t utiliser <code>external-secrets</code></p> <p>On cr\u00e9e un <code>ExternalSecret</code> </p> <pre><code>---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: external-secret-gitlabcred\n  namespace: kyverno\nspec:\n  refreshInterval: 1m\n  secretStoreRef:\n    name: gitlab-cluster-secret-store\n    kind: ClusterSecretStore\n  target:\n    name: gitlabcred\n    creationPolicy: Owner\n    template:\n      engineVersion: v2\n      type: kubernetes.io/dockerconfigjson # (1)\n      data: # (2)\n        .dockerconfigjson: \"{\\\"auths\\\":{\\\"registry.gitlab.com\\\":{\\\"username\\\":\\\"{{ .username }}\\\",\\\"password\\\":\\\"{{ .password }}\\\",\\\"auth\\\":\\\"{{(printf \\\"%s:%s\\\" .username .password) | b64enc }}\\\"}}}\"\n  data: # (3)\n    - secretKey: username\n      remoteRef:\n        key: gl_cr_username\n    - secretKey: password\n      remoteRef:\n        key: gl_cr_password\n</code></pre> <ol> <li>\ud83d\udc33 On d\u00e9finit le type de template que l'on utilise pour cr\u00e9er le secret</li> <li>\ud83d\udcbd On cr\u00e9e le dockerconfigjson \u00e0 partir de 2 variables</li> <li>\ud83e\udd8a On r\u00e9cup\u00e8re le username &amp; le mot de passe depuis GitLab</li> </ol> <p>que l'on d\u00e9ploie</p> <pre><code>kubectl apply -f kyverno/kyverno-gitlabcred-external-secret.yml\n</code></pre> <p>Secret correctement cr\u00e9\u00e9</p> <p>L'<code>ExternalSecret</code> est cr\u00e9\u00e9 et synchronis\u00e9</p> <p><pre><code>kubectl get externalsecrets.external-secrets.io -n kyverno\n</code></pre> <pre><code>NAME                         STORE                         REFRESH INTERVAL   STATUS         READY\nexternal-secret-gitlabcred   gitlab-cluster-secret-store   1m                 SecretSynced   True\n</code></pre></p> <p>et le secret aussi <pre><code>kubectl describe secret gitlabcred -n kyverno\n</code></pre> <pre><code>Name:         gitlabcred\nNamespace:    kyverno\nLabels:       reconcile.external-secrets.io/created-by=25390d6b8b839ba8a1d72cfcfe6f6319\nAnnotations:  reconcile.external-secrets.io/data-hash: 9dfdd28d70dbacf25d05ab5d782ae9a5\n\nType:  kubernetes.io/dockerconfigjson\n\nData\n====\n.dockerconfigjson:  154 bytes\n</code></pre></p> <p>On doit pouvoir d\u00e9ployer notre nouvelle image</p> <pre><code>kubectl apply -f kyverno/kyverno-asciinematic-hardened.yml\n</code></pre> Pour les curieux, le fichier <code>kyverno-asciinematic-hardened.yml</code> <pre><code>\n</code></pre> <p>Image d\u00e9ploy\u00e9e</p> <pre><code>kubectl get po -n demos\n</code></pre> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\nasciinematic-hardened-95b5f9d76-b87j9   1/1     Running   0          64s\n</code></pre>"},{"location":"kyverno/#une-2eme-policy","title":"Une 2\u00e8me policy","text":"<p>Du fait que l'on a forc\u00e9 sur l'ensemble du cluster que les images proviennent de GitLab, il serait pratique qu'automatiquement lorsque l'on cr\u00e9e un namespace, automatiquement le secret pour se connecter \u00e0 GitLab via <code>external-secrets</code> se cr\u00e9e.</p> <p>On a utilis\u00e9 un type <code>validate</code> lors de notre premi\u00e8re policy, dans ce second cas, on va utiliser le type <code>generate</code> qui va automatiquement g\u00e9n\u00e9rer des \u00e9l\u00e9ments.</p> <pre><code># ...\n  policies.kyverno.io/description: &gt;-\n    Secrets like registry credentials often need to exist in multiple\n    Namespaces so Pods there have access. Manually duplicating those Secrets\n    is time consuming and error prone. This policy will copy a\n    Secret called `gitlabcred` which exists in the `kyverno` Namespace to\n    new Namespaces when they are created. It will also push updates to\n    the copied Secrets should the source Secret be changed.\n  spec:\n    rules:\n      - name: sync-image-pull-secret\n        # ...\n        generate: # (1)\n          apiVersion: v1\n          kind: Secret\n          name: regcred\n          namespace: \"{{request.object.metadata.name}}\"\n          synchronize: true\n          clone: # (2)\n            namespace: kyverno\n            name: gitlabcred\n</code></pre> <ol> <li>\ud83d\udc7d Type <code>generate</code></li> <li>\ud83e\uddec Action de clone d'un object existant</li> </ol> <p>D\u00e9ployons cette nouvelle policy</p> <pre><code>kubectl apply -f kyverno/kyverno-sync-regcred.yml\n</code></pre> Pour les curieux, le fichier <code>kyverno-sync-regcred.yml</code> <pre><code>\n</code></pre> <p>On v\u00e9rifie que la policy est cr\u00e9\u00e9e et op\u00e9rationnelle</p> <pre><code>kubectl get clusterpolicies.kyverno.io -n kyverno\n</code></pre> <p>Policy cr\u00e9\u00e9e et op\u00e9rationnelle</p> <pre><code>NAME                      ADMISSION   BACKGROUND   VALIDATE ACTION   READY   AGE   MESSAGE\nrestrict-image-registry   true        true         Enforce           True    66m   Ready # (1)\nsync-secrets              true        true         Audit             True    26s   Ready\n</code></pre> <ol> <li>\ud83d\ude94 On retrouve bien notre premi\u00e8re policy</li> </ol>"},{"location":"kyverno/#verification_1","title":"V\u00e9rification","text":"<p>V\u00e9rifions que notre nouvelle policy fonctionne bien en cr\u00e9ant un nouveau namespace</p> <pre><code>kubectl create ns demo-kyverno\n</code></pre> <p>Normalement, on devrait avoir un secret dans notre nouveau namespace</p> <pre><code>kubectl get secret -n demo-kyverno\n</code></pre> <p>Secret cr\u00e9\u00e9</p> <pre><code>NAME         TYPE                          DATA   AGE\nregcred   kubernetes.io/dockerconfigjson   1      112s\n</code></pre> <p>Voyons si l'on essaie de d\u00e9ployer une image provenant de notre registry priv\u00e9e dans notre nouveau namespace</p> <pre><code>kubectl run demo-nginx --image=registry.gitlab.com/yodamad-workshops/kub-workshop/nginx:hardened -n demo-kyverno\n</code></pre> <p>Image d\u00e9ploy\u00e9e</p> <pre><code>kubectl get po -n demo-kyverno\n</code></pre> <pre><code>NAME         READY   STATUS    RESTARTS   AGE\ndemo-nginx   1/1     Running   0          15s\n</code></pre> <p>Et v\u00e9rifions que notre premi\u00e8re policy est bien valable dans notre nouveau namespace</p> <pre><code>kubectl run demo-nginx --image=nginx:latest -n demo-kyverno\n</code></pre> <p>Impossible de d\u00e9ployer</p> <pre><code>Error from server: admission webhook \"validate.kyverno.svc-fail\" denied the request:\n\nresource Pod/demo-kyverno/demo-nginx was blocked due to the following policies\n\nrestrict-image-registry:\n  validate-registries: 'validation error: Unauthorized registry. rule validate-registries\n    failed at path /spec/containers/0/image/'\n</code></pre>"},{"location":"kyverno/#suivi-de-nos-policies","title":"Suivi de nos policies","text":"<p>Kyverno permet de facilement voir les policies qui ont \u00e9t\u00e9 ex\u00e9cut\u00e9es</p> <pre><code>kubectl get policyreports -n demo-kyverno -o wide\n</code></pre> <p>La policy a \u00e9t\u00e9 ex\u00e9cut\u00e9e avec succ\u00e8s</p> <pre><code>NAME                                   KIND   NAME         PASS   FAIL   WARN   ERROR   SKIP   AGE\n3e3ad989-01be-46fc-afc9-7eea0f78a74c   Pod    demo-nginx   1      0      0      0       0      5m9s\n</code></pre> <p>On peut aussi v\u00e9rifier \u00e0 l'\u00e9chelle du cluster</p> <pre><code>kubectl get policyreports -A\n</code></pre> L'ensemble des reports du cluster <pre><code>NAMESPACE      NAME                                   PASS   FAIL   WARN   ERROR   SKIP   AGE\ndemo-kyverno   3e3ad989-01be-46fc-afc9-7eea0f78a74c   1      0      0      0       0      14m\ndemos          4649657f-4994-47eb-a15b-a3e2b14e82db   0      1      0      0       0      88m\ndemos          718a97de-06af-4df8-b7d0-17f7ca9b5d02   1      0      0      0       0      34m\ndemos          81d369ad-0b5a-46ba-85b6-0e73e7572afe   0      1      0      0       0      88m\n</code></pre> <p>On voit que les policies se sont ex\u00e9cut\u00e9es sur les composants install\u00e9s avant la policy et que certains sont aussi status <code>FAIL</code></p>"},{"location":"kyverno/#pour-aller-plus-loin","title":"Pour aller plus loin","text":"<p>Kyverno propose d'autres types de policies.</p> Article sur le sujet #autopromo <p>Un article d\u00e9crivant plus en d\u00e9tails cela est dispo sur le blog</p> <p>Notre cocotte est s\u00e9curis\u00e9e et avec des bons produits issus de la fili\u00e8re locale \ud83d\ude09</p> <p>Il est temps de se reposer un peu \u27a1\ufe0f</p>"},{"location":"nginx-ingress-controller/","title":"Controller nos Ingress","text":"<p>Pour nous aider \u00e0 g\u00e9rer nos Ingress, ingress-nginx-controller va nous aider pour automatiquement faire le routage depuis l'ext\u00e9rieur.</p> <p>D'abord, il faut ajouter le repo Helm pour le chart:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\n</code></pre> <p>Ensuite, on peut installer le chart: (1)</p> <ol> <li>\u2139\ufe0f l'option <code>create-namespace</code> force la cr\u00e9ation d'un nouveau namespace</li> </ol> <pre><code>helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \\\n  --namespace nginx-ingress-controller \\\n  --create-namespace \\\n  --set controller.publishService.enabled=true\n</code></pre> <p>On peut v\u00e9rifier que tout est install\u00e9 correctement</p> <pre><code>helm ls -A\n</code></pre> <p>Helm install\u00e9</p> <pre><code>NAME            NAMESPACE                   REVISION    UPDATED                                 STATUS      CHART               APP VERSION\ningress-nginx   nginx-ingress-controller    1           2023-12-19 15:33:53.505518 +0100 CET    deployed    ingress-nginx-4.8.4 1.9.4       # (1)\n</code></pre> <ol> <li>La version <code>1.9.4</code> peut vari\u00e9e</li> </ol> <p>On peut v\u00e9rifier que le namespace et les composants sont bien cr\u00e9\u00e9s :</p> <pre><code>kubectl get ns\n</code></pre> <p>Le namespace est cr\u00e9\u00e9</p> <pre><code>NAME                       STATUS   AGE\ndefault                    Active   5h17m\nkube-node-lease            Active   5h17m\nkube-public                Active   5h17m\nkube-system                Active   5h17m\nnginx-ingress-controller   Active   10s\n</code></pre> <pre><code>kubectl get all -n nginx-ingress-controller\n</code></pre> <p>Les composants sont instanci\u00e9s et op\u00e9rationnels</p> <pre><code>NAME                                            READY   STATUS    RESTARTS   AGE\npod/ingress-nginx-controller-75967d99c9-9vcwr   1/1     Running   0          2m45s\n\nNAME                                         TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)                      AGE\nservice/ingress-nginx-controller             LoadBalancer   10.3.28.249   57.128.120.49   80:31253/TCP,443:31582/TCP   2m46s\nservice/ingress-nginx-controller-admission   ClusterIP      10.3.10.52    &lt;none&gt;          443/TCP                      2m46s\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ingress-nginx-controller   1/1     1            1           2m46s\n\nNAME                                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ingress-nginx-controller-75967d99c9   1         1         1       2m46s\n</code></pre> <p>Info</p> <p>Cela peut prendre un peu de temps avant que l'IP externe soit disponible.</p> <p>Une fois l'IP externe disponible, on peut facilement la r\u00e9cup\u00e9rer:</p> <pre><code>export EXT_IP=$(kubectl get service ingress-nginx-controller -n nginx-ingress-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\necho ${EXT_IP:-NOT_SET_WAIT_AND_RETRY}\n</code></pre> <p>D\u00e9sormais, notre <code>ingress-controller</code> va automatiquement g\u00e9rer le routage pour nous lors de l'ajout d'<code>Ingress</code> dans le cluster.</p> <p>Pour cela, il faut d\u00e9finir un <code>Ingress</code> avec le <code>Deployment</code> pr\u00e9cisant l'URL sur laquelle exposer le service et sp\u00e9cifier dans l'attribut <code>host</code> l'URL sur laquelle on souhaite exposer notre deployment.</p> Exemple avec le premier d\u00e9ploiement <p>(pas besoin de le faire, on l'a pr\u00e9par\u00e9 pour vous, cf. la suite \ud83d\ude09)</p> <p>On pourrait ajouter cela \u00e0 notre 1er d\u00e9ploiement, par exemple: </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: simple-deployment-ingress\n  namespace: demos\n  annotations:\n    external-dns.alpha.kubernetes.io/cloudflare-proxied: \"false\" # (1)\nspec:\n  ingressClassName: nginx\n  rules:\n    - host:  simple-deployment.&lt;votre_trigramme&gt;.grunty.uk # (2)\n      http:\n        paths:\n          - backend:\n              service:\n                name: simple-deployment-service\n                port:\n                  number: 80\n            pathType: Prefix\n            path: /\n---\n</code></pre> <ol> <li>\u2139\ufe0f Sp\u00e9cificit\u00e9 lorsque l'on utilise cloudflare</li> <li>\ud83d\udedc URL sur laquelle on veut exposer le service</li> </ol> <p>On va installer un nouveau <code>Deployment</code> avec son <code>Ingress</code> gr\u00e2ce au fichier <code>demos/deployment-with-ingress-grunty.yml</code></p> <p>Pensez \u00e0 mettre votre trigramme</p> <p>Pensez bien \u00e0 modifier <code>demos/deployment-with-ingress-grunty.yml</code> avec votre trigramme avant de faire le <code>apply</code></p> <pre><code>kubectl apply -f demos/deployment-with-ingress-grunty.yml\nexport my_host=new-deployment.&lt;votre_trigramme&gt;.grunty.uk\necho \"Site URL http://\"${my_host}\n</code></pre> Pour les curieux, le contenu de <code>deployment-with-ingress-grunty.yml</code> <pre><code>\n</code></pre> <p>Demo d\u00e9ploy\u00e9e</p> <p>Le pod est d\u00e9marr\u00e9 : <pre><code>kubectl get po -n demos\n</code></pre> donne <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\nnew-deployment-5998d8dbcc-kdbrk     1/1     Running   0          9m21s\nsimple-deployment-5897799cb-94xzv   1/1     Running   0          58m\n</code></pre></p> <p>et l'ingress cr\u00e9\u00e9 avec la bonne url: <pre><code>kubectl get ingress -n demos\n</code></pre> donne <pre><code>NAME                     CLASS   HOSTS                          ADDRESS         PORTS   AGE\nnew-deployment-ingress   nginx   new-deployment.mvt.grunty.uk   57.128.120.31   80      9m32s\n</code></pre></p> <p>Gr\u00e2ce \u00e0 l'ingress controller, nous n'avons plus besoin de faire le <code>port-forward</code> comme pr\u00e9c\u00e9demment car le routage est fait sur l'URL. On peut donc tester d'acc\u00e9der \u00e0 nos pods via <code>curl</code></p> <p>2\ufe0f\u20e3 m\u00e9thodes possibles:</p> <ul> <li>En forcant un header <code>Host</code> avec l'URL voulue:</li> </ul> <p><pre><code>echo \"Host: ${my_host}\" \"http://${EXT_IP}/\"\ncurl --header \"Host: ${my_host}\" \"http://${EXT_IP}/\"\n</code></pre> * ou, en surchargeant la r\u00e9solution DNS dynamiquement:</p> <pre><code>curl --resolve ${my_host}:80:${EXT_IP} -H \"Host: ${my_host}\" -i \"http://${EXT_IP}\"\n</code></pre> <p>Les 2 commandes donnent le m\u00eame r\u00e9sultat: (1)</p> <ol> <li>\u23f3 La commande avec l'option <code>--resolve</code> est l\u00e9g\u00e8rement plus verbeuse</li> </ol> <pre><code>Server address: 10.2.1.6:80\nServer name: new-deployment-5998d8dbcc-kdbrk\nDate: 19/Dec/2023:15:07:19 +0000\nURI: /\nRequest ID: 4246cc4f7d0d56a49792cc73024dc779\n</code></pre> <p>On a chang\u00e9 l'image Docker entre temps</p> <p>Ne vous \u00e9tonnez pas si vous ne voyez plus le magnifique cin\u00e9ma ASCII du d\u00e9part.</p> <p>On a chang\u00e9 l'image Docker pour que ce que l'on affiche soit lisible lorsque l'on fait un cUrl</p> <p>Ok ca fonctionne, mais ca reste pas super pratique \u00e0 utiliser \ud83e\udee4 car m\u00eame si l'on a d\u00e9fini une URL dans notre manifest, celle-ci n'est pas r\u00e9f\u00e9renc\u00e9e dans un DNS.</p> <p>\ud83d\udca1 Ca serait pratique, si automatiquement nos entr\u00e9es DNS \u00e9taient cr\u00e9es et publi\u00e9es pour que ce soit accessible depuis un navigateur comme tout autre site \"classique\".</p> <p>Et si <code>external-dns</code> \u00e9tait la solution \u2049\ufe0f</p> <p>\ud83e\uddc2 Alors ajoutons-en une pinc\u00e9e pour voir \u27a1\ufe0f</p>"},{"location":"terraform/","title":"Cr\u00e9er son cluster avec Terraform","text":"<p>D'abord, il faut initialiser l'environnement terraform <pre><code>cd terraform # (1)\nterraform init\n</code></pre></p> <ol> <li>On doit se mettre dans le r\u00e9pertoire <code>terraform</code> pour r\u00e9aliser les \u00e9tapes</li> </ol> <p>Terraform initialis\u00e9 correctement</p> <p>La commande se termine par Terraform has been successfully initialized!</p> <p>Il faut donner un nom \u00e0 votre cluster:</p> <pre><code>export TF_VAR_OVH_CLOUD_PROJECT_KUBE_NAME=&lt;votre_trigramme&gt; # (1)\n</code></pre> <ol> <li>\ud83d\udea8 Mettre un trigramme a minima, voire un chiffre \u00e9galement pour l'unicit\u00e9 des clusters</li> </ol> <p>\ud83d\udea8 Attention \u00e0 bien respecter les r\u00e8gles de nommage: </p> <ul> <li>Pas d'underscore, pas d'espace, pas de majuscule, pas d'accent, pas de caract\u00e8res sp\u00e9ciaux </li> <li>RIEN !! que des minuscules et - \ud83d\ude0a (regex: '^[a-z0-9]([-a-z0-9]*[a-z0-9])?$')</li> </ul> Variables n\u00e9cessaires pour initialiser le cluster sur OVH <p>Ensuite, il faut configurer des variables d'environnements pour int\u00e9ragir avec notre cloud provider:</p> <pre><code>export TF_VAR_SERVICE_NAME=\"\"\nexport TF_VAR_APPLICATION_KEY=\"\"\nexport TF_VAR_APPLICATION_SECRET=\"\"\nexport TF_VAR_CONSUMER_KEY=\"\"\n</code></pre> <p>On est sympa, c'est d\u00e9j\u00e0 fait gr\u00e2ce au script ex\u00e9cut\u00e9 au d\u00e9but du workshop. Si vous avez changer de terminal, il faut refaire la commande suivante: <pre><code>\n</code></pre></p> <p>D'autres variables d\u00e9crites dans <code>variables.tf</code> peuvent \u00eatre surcharg\u00e9es comme par exemple la taille des noeuds ou la localisation du cluster. Mais ...</p> <p>Horacio Gonz\u00e1lez (@LostInBrittany)</p> <p>Le GRA : c'est la vie ! \ud83e\uddc8</p> <p>donc par d\u00e9faut nous utilisons le datacenter de Gravelines.</p> Pour les curieux <code>variables.tf</code> <pre><code>\n</code></pre> <p>On peut maintenant \"planifier\" notre d\u00e9ploiement:</p> <pre><code>terraform plan -out kub-workshop.plan\n</code></pre> <p>Terraform correctement planifi\u00e9</p> <p>Aucune erreur n'apparait dans la console.</p> <p>On peut d\u00e9sormer ex\u00e9cuter le d\u00e9ploiement</p> <pre><code>terraform apply kub-workshop.plan\n</code></pre> A propos de <code>kub-workshop.plan</code> <p>On peut voir qu'un fichier <code>kub-workshop.plan</code> a \u00e9t\u00e9 cr\u00e9\u00e9 \u00e0 la racine de notre repo. Il s'agit d'un fichier binaire contenant les informations n\u00e9cessaires \u00e0 Terraform pour int\u00e9ragir avec le provider.</p> <p>L'ex\u00e9cution prend une dizaine de minutes, le temps de prendre un \u2615\ufe0f car apr\u00e8s c'est parti !!</p> <p>Cluster cr\u00e9\u00e9</p> <p>Apply complete! Resources: 2 added, 0 changed, 0 destroyed.</p> <p>Maintenant, il faut r\u00e9cup\u00e9rer notre fichier <code>kubeconfig</code> pour int\u00e9ragir avec notre nouveau cluster:</p> <pre><code>terraform output -raw kubeconfig &gt; cluster-ovh-${TF_VAR_OVH_CLOUD_PROJECT_KUBE_NAME}.yml\nexport KUBECONFIG=$(pwd)/cluster-ovh-${TF_VAR_OVH_CLOUD_PROJECT_KUBE_NAME}.yml\n</code></pre> <p>V\u00e9rifier que la connexion est ok:</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Le r\u00e9sultat ressemble \u00e0: (1)</p> <ol> <li>La version peut diff\u00e9r\u00e9e par rapport \u00e0 l'exemple</li> </ol> <pre><code>NAME                            STATUS   ROLES    AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nmvt-snowcamp-pool-node-47dd14   Ready    &lt;none&gt;   20m   v1.27.8   57.128.56.219   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-91-generic   containerd://1.6.25\n</code></pre> <p>\ud83d\udcab Notre cluster est pr\u00eat, d\u00e9ployons notre premi\u00e8re application \u27a1\ufe0f</p>"},{"location":"terraform/en_README/","title":"Set up cluster with Terraform","text":"<p>First of all, we need to create a cluster. We will use Terraform to do it.</p> <p>First, init Terraform stat <pre><code>terraform init\n</code></pre></p> <p>then set up some env. variables to access OVH environment <pre><code>export SERVICE_NAME=\"\"\nexport APPLICATION_KEY=\"\"\nexport APPLICATION_SECRET=\"\"\nexport CONSUMER_KEY=\"\"\nexport OVH_CLOUD_PROJECT_KUBE_NAME=\"\"\n</code></pre> We can also use terraform.tfvars file to populate terraform variables</p> <p>Now we are ready to check Terraform plan</p> <pre><code>terraform plan -out my.plan\n</code></pre> <p>and launch it !</p> <p><pre><code>terraform apply my.plan\n</code></pre> During this time go grab a \u2615 </p> <p>When <code>apply</code> is done, we can retrieve our kubeconfig file to be able to connect to the cluster</p> <pre><code>source terraform.tfvars\nterraform output -raw kubeconfig &gt; cluster-ovh-${OVH_CLOUD_PROJECT_KUBE_NAME}.yml\nexport KUBECONFIG=./cluster-ovh-${OVH_CLOUD_PROJECT_KUBE_NAME}.yml\n[ -d ../../../.kube/custom-contexts ] &amp;&amp; cp \"./cluster-ovh-${OVH_CLOUD_PROJECT_KUBE_NAME}.yml\" ../../../.kube/custom-contexts/\n</code></pre> <pre><code>kubectl get nodes  -o wide\nkubectl get ns\n</code></pre> <p>Now, let's deploy something, a simple <code>Deployment</code></p> <pre><code>kubectl apply -f ../demos/simple-deployment.yml\n</code></pre> <p><pre><code>kubectl get deployments -n demos\n</code></pre> And try to access it :</p> <pre><code>echo http://localhost:8080\nkubectl -n demos port-forward $(kubectl get pods -o=name -n demos) 8080:80\n</code></pre> <p>and visit http://localhost:8080  \ud83c\udf86</p> <p>BUT:   * we can't access it from outside  * you must map pod port to host port</p> <p>If you want to access it from outside, you need to create a <code>Service</code> and an <code>Ingress</code> and add an ingress controller. See you in the next chapter !</p> <p>To delete this cluster use  <pre><code>terraform destroy \n</code></pre></p>"}]}